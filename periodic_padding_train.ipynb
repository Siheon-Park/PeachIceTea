{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.optim import Adam, Adagrad\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "SEED = 16\n",
    "TRAINSIZE = 10000\n",
    "VALSIZE = 1000\n",
    "TESTSIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QCNN_circuit_only import QCNN\n",
    "from ansatz import ConvCirc1, PoolingCirc\n",
    "\n",
    "INPUT_QUBITS = 8\n",
    "\n",
    "qcnn = QCNN(n_qubits=INPUT_QUBITS, conv_ansatz=ConvCirc1(), pooling_ansatz=PoolingCirc(), stride=1)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=qcnn.total_qubit)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def circuit(params, data):\n",
    "    qcnn.construct_circuit(params, data)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Complete\n",
      "Training Data Processing Complete\n",
      "Test Data Processing Complete\n",
      "Training set has 10000 instances\n",
      "validation set has 1000 instances\n",
      "test set has 1000 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quic/Ju-Young Ryu/PeachIceTea-1/datasetloading.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data[i] = self.data[int((num_added_columns % data_dim))]\n"
     ]
    }
   ],
   "source": [
    "from datasetloading import TrainTestLoader\n",
    "\n",
    "loader = TrainTestLoader('./mnist_train.csv', './mnist_test.csv')\n",
    "loader.scale_pca(n_dim=30)\n",
    "training_set, test_set = loader.return_dataset()\n",
    "print('PCA Complete')\n",
    "\n",
    "training_set.filter([0, 1])\n",
    "training_set.periodic_data_padding()\n",
    "print('Training Data Processing Complete')\n",
    "test_set.filter([0, 1])\n",
    "test_set.periodic_data_padding()\n",
    "print('Test Data Processing Complete')\n",
    "\n",
    "training_set, validation_set = random_split(training_set, [TRAINSIZE, len(training_set)-TRAINSIZE], generator=torch.Generator().manual_seed(SEED))\n",
    "validation_set, _ = random_split(validation_set, [VALSIZE, len(validation_set)-VALSIZE], generator=torch.Generator().manual_seed(SEED))\n",
    "test_set, _ = random_split(test_set, [TESTSIZE, len(test_set)-TESTSIZE], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for test\n",
    "training_loader = DataLoader(training_set, batch_size=25, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "validation_loader = DataLoader(validation_set, batch_size=25, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "test_loader = DataLoader(test_set, batch_size=25, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('validation set has {} instances'.format(len(validation_set)))\n",
    "print('test set has {} instances'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "loss_fn = MSELoss() # TODO: choose loss function\n",
    "params = torch.rand(qcnn.Calculate_Param_Num(), requires_grad=True)\n",
    "\n",
    "optimizer = Adam([params], lr=0.01) # TODO: choose optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quic/anaconda3/envs/ddi/lib/python3.9/site-packages/torch/autograd/__init__.py:154: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180549130/work/aten/src/ATen/native/Copy.cpp:244.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "loggername = f'{str(datetime.datetime.now())}'\n",
    "filepath = Path(f'./experiment_results/{loggername}')\n",
    "filepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def estimated_label(params, data):\n",
    "    return circuit(params, data) # TODO: customize your extimated label\n",
    "\n",
    "writer = SummaryWriter(log_dir='./PeriodicPadding')\n",
    "\n",
    "loss_list = []\n",
    "avg_loss_list = []\n",
    "avg_vloss_list = []\n",
    "history_of_parameters = []\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.tensor(0, dtype=torch.float64)\n",
    "        for input, label in zip(inputs, labels):\n",
    "            output = estimated_label(params, input.to(torch.float64))\n",
    "            loss+=loss_fn(output, 2*label.to(torch.float64)-1)/len(labels)\n",
    "        loss.backward()       \n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        loss_list.append(loss.item())\n",
    "        writer.add_scalar('local training loss', loss.item(), epoch*len(training_loader)+i)\n",
    "        np.save(filepath/f'epoch{epoch}_iter{epoch*len(training_loader)+i}', params.detach().numpy())\n",
    "        # logger.debug(\"epoch: {} batch_index: {} --- loss: {} \\t params: {}\".format(epoch+1, i+1, loss.item(), params.detach().numpy()))\n",
    "\n",
    "\n",
    "    avg_loss = avg_loss/len(training_loader)\n",
    "    avg_loss_list.append(avg_loss)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        avg_vloss = 0.0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vloss = torch.tensor(0, dtype=torch.float64)\n",
    "            vinputs, vlabels = vdata\n",
    "            for vinput, vlabel in zip(vinputs, vlabels):\n",
    "                voutput = estimated_label(params, vinput.to(torch.float64))\n",
    "                vloss += loss_fn(voutput, 2*vlabel.to(torch.float64)-1)/len(vlabels)\n",
    "            avg_vloss += vloss.item()\n",
    "        avg_vloss = avg_vloss/len(validation_loader)\n",
    "        avg_vloss_list.append(avg_vloss)\n",
    "        history_of_parameters.append(params.numpy())\n",
    "        np.save(filepath/f'vaidataion_epoch{epoch}', params.numpy())\n",
    "    # print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and test\n",
    "    writer.add_scalar('validation Loss', avg_vloss, epoch)\n",
    "    writer.flush()\n",
    "        # print('{}: {}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946\n"
     ]
    }
   ],
   "source": [
    "# Calcaulate Test Accuracy\n",
    "est_labels = []\n",
    "true_labels = []\n",
    "for i, data in enumerate(test_loader):\n",
    "    test_datas, labels = data\n",
    "    for label, test_data in zip(labels, test_datas):\n",
    "        est_labels.append(estimated_label(params, test_data)>0)\n",
    "        true_labels.append(label)\n",
    "\n",
    "accuracy = sum(np.array(est_labels)==np.array(true_labels))/len(est_labels)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ddi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c098d66d23913e0ec4673b62bea35be032f141ba022765f23c8046c187af3be1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
