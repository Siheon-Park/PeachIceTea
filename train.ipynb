{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.optim import Adam, Adagrad\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "SEED = 16\n",
    "TRAINSIZE = 100\n",
    "VALSIZE = 100\n",
    "TESTSIZE = 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QCNN_circuit_only import QCNN\n",
    "from ansatz import ConvCirc1, PoolingCirc\n",
    "\n",
    "qcnn = QCNN(n_qubits=8, conv_ansatz=ConvCirc1(), pooling_ansatz=PoolingCirc(), stride=1)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=qcnn.total_qubit)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def circuit(params, data):\n",
    "    qcnn.construct_circuit(params, data)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetloading import TrainTestLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 100 instances\n",
      "validation set has 100 instances\n",
      "test set has 115 instances\n"
     ]
    }
   ],
   "source": [
    "loader = TrainTestLoader('./mnist_train.csv', './mnist_test.csv')\n",
    "loader.scale_pca(n_dim=30)\n",
    "training_set, test_set = loader.return_dataset()\n",
    "training_set.filter([0, 1])\n",
    "training_set, validation_set = random_split(training_set, [TRAINSIZE, len(training_set)-TRAINSIZE], generator=torch.Generator().manual_seed(SEED))\n",
    "validation_set, _ = random_split(validation_set, [VALSIZE, len(validation_set)-VALSIZE], generator=torch.Generator().manual_seed(SEED))\n",
    "test_set, _ = random_split(test_set, [TESTSIZE, len(test_set)-TESTSIZE], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for test\n",
    "training_loader = DataLoader(training_set, batch_size=25, shuffle=True, generator=torch.Generator().manual_seed(SEED))\n",
    "validation_loader = DataLoader(validation_set, batch_size=25, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "test_loader = DataLoader(test_set, batch_size=25, shuffle=False, generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('validation set has {} instances'.format(len(validation_set)))\n",
    "print('test set has {} instances'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSELoss() # TODO: choose loss function\n",
    "params = torch.rand(qcnn.Calculate_Param_Num(), requires_grad=True) # TODO: choose params length\n",
    "\n",
    "optimizer = Adam([params], lr=0.01) # TODO: choose optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaf29fe8c0b401e9acd0cb76f7ad9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcd3505ff9d4ed097f83cba91752675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48289dce27c143a1a95f99814ef5e1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a41c29bc384e5aade8b0eb16db15b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "4:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5459c972aa401caad3d37c160dff09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "5:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8448dca26d47ec8c7f22678d3012b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "6:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77383e483f954cf9882160cf5abab74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296b244b81a74019845fd21d435c3e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "8:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "loggername = f'{str(datetime.datetime.now())}'\n",
    "filepath = Path(f'./experiment_results/{loggername}')\n",
    "filepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def estimated_label(params, data):\n",
    "    return circuit(params, data) # TODO: customize your extimated label\n",
    "\n",
    "writer = SummaryWriter(log_dir='./runs')\n",
    "\n",
    "loss_list = []\n",
    "avg_loss_list = []\n",
    "avg_vloss_list = []\n",
    "history_of_parameters = []\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(training_loader), total=len(training_loader), desc=f\"{epoch+1}\"):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.tensor(0, dtype=torch.float64)\n",
    "        for input, label in zip(inputs, labels):\n",
    "            output = estimated_label(params, input.to(torch.float64))\n",
    "            loss+=loss_fn(output, 2*label.to(torch.float64)-1)/len(labels)\n",
    "        loss.backward()       \n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        loss_list.append(loss.item())\n",
    "        writer.add_scalar('local training loss', loss.item(), epoch*len(training_loader)+i)\n",
    "        np.save(filepath/f'epoch{epoch}_iter{epoch*len(training_loader)+i}', params.detach().numpy())\n",
    "        # logger.debug(\"epoch: {} batch_index: {} --- loss: {} \\t params: {}\".format(epoch+1, i+1, loss.item(), params.detach().numpy()))\n",
    "\n",
    "\n",
    "    avg_loss = avg_loss/len(training_loader)\n",
    "    avg_loss_list.append(avg_loss)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        avg_vloss = 0.0\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            vloss = torch.tensor(0, dtype=torch.float64)\n",
    "            vinputs, vlabels = vdata\n",
    "            for vinput, vlabel in zip(vinputs, vlabels):\n",
    "                voutput = estimated_label(params, vinput.to(torch.float64))\n",
    "                vloss += loss_fn(voutput, 2*vlabel.to(torch.float64)-1)/len(vlabels)\n",
    "            avg_vloss += vloss.item()\n",
    "        avg_vloss = avg_vloss/len(test_loader)\n",
    "        avg_vloss_list.append(avg_vloss)\n",
    "        history_of_parameters.append(params.numpy())\n",
    "        np.save(filepath/f'vaidataion_epoch{epoch}', params.numpy())\n",
    "    # print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and test\n",
    "    writer.add_scalar('validation Loss', avg_vloss, epoch)\n",
    "    writer.flush()\n",
    "        # print('{}: {}'.format(i, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    est_labels = []\n",
    "    true_labels = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_datas, labels = data\n",
    "        for label, test_data in zip(labels, test_datas):\n",
    "            est_labels.append(estimated_label(params, test_data).numpy()>0)\n",
    "            true_labels.append(label.numpy())\n",
    "\n",
    "    accuracy = sum(np.array(est_labels)==np.array(true_labels))/len(est_labels)\n",
    "    accuracy = accuracy.item()\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(filepath/'summary.json', 'w') as fp:\n",
    "    json.dump(dict(\n",
    "        accuracy=accuracy,\n",
    "        loss=loss_list,\n",
    "        avg_loss = avg_loss_list,\n",
    "        avg_vloss = avg_vloss_list,\n",
    "    ), fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62b6089cd174fcd23de5a2ba1f03f98164a35935839c99f5cfe650d529eeaec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('qiskit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
