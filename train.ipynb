{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.optim import Adam, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=2)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def circuit(phi, theta, data:torch.Tensor):\n",
    "    qml.AmplitudeEmbedding(data.flatten()[:4], wires=(0, 1), pad_with=0, normalize=True)\n",
    "    qml.RX(phi[0], wires=0)\n",
    "    qml.RY(phi[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.PhaseShift(theta, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0)), qml.expval(qml.Hadamard(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 60000 instances\n",
      "Validation set has 10000 instances\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('Validation set has {} instances'.format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trouser  Coat  T-shirt/top  Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "print('  '.join(classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSELoss()\n",
    "phi = torch.rand(2, requires_grad=True, dtype=torch.float)\n",
    "theta = torch.rand(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "optimizer = Adam([phi, theta], lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66.0410, grad_fn=<ToCopyBackward0>)\n",
      "0: 66.04104614257812\n",
      "tensor(91.3534, grad_fn=<ToCopyBackward0>)\n",
      "1: 91.35338592529297\n",
      "tensor(62.0264, grad_fn=<ToCopyBackward0>)\n",
      "2: 62.02635955810547\n",
      "tensor(107.6202, grad_fn=<ToCopyBackward0>)\n",
      "3: 107.62018585205078\n",
      "tensor(59.3473, grad_fn=<ToCopyBackward0>)\n",
      "4: 59.347293853759766\n",
      "tensor(154.4350, grad_fn=<ToCopyBackward0>)\n",
      "5: 154.43499755859375\n",
      "tensor(197.0269, grad_fn=<ToCopyBackward0>)\n",
      "6: 197.02694702148438\n",
      "tensor(103.5136, grad_fn=<ToCopyBackward0>)\n",
      "7: 103.51361083984375\n",
      "tensor(97.1755, grad_fn=<ToCopyBackward0>)\n",
      "8: 97.17550659179688\n",
      "tensor(123.0262, grad_fn=<ToCopyBackward0>)\n",
      "9: 123.02616119384766\n",
      "tensor(83.0569, grad_fn=<ToCopyBackward0>)\n",
      "10: 83.05691528320312\n",
      "tensor(159.5858, grad_fn=<ToCopyBackward0>)\n",
      "11: 159.58584594726562\n",
      "tensor(52.7866, grad_fn=<ToCopyBackward0>)\n",
      "12: 52.786624908447266\n",
      "tensor(74.7702, grad_fn=<ToCopyBackward0>)\n",
      "13: 74.77021026611328\n",
      "tensor(82.2807, grad_fn=<ToCopyBackward0>)\n",
      "14: 82.28069305419922\n",
      "tensor(138.8684, grad_fn=<ToCopyBackward0>)\n",
      "15: 138.86843872070312\n",
      "tensor(16.6063, grad_fn=<ToCopyBackward0>)\n",
      "16: 16.606277465820312\n",
      "tensor(118.8278, grad_fn=<ToCopyBackward0>)\n",
      "17: 118.82781982421875\n",
      "tensor(168.0136, grad_fn=<ToCopyBackward0>)\n",
      "18: 168.01361083984375\n",
      "tensor(68.9915, grad_fn=<ToCopyBackward0>)\n",
      "19: 68.99150848388672\n",
      "tensor(5.9875, grad_fn=<ToCopyBackward0>)\n",
      "20: 5.9874653816223145\n",
      "tensor(48.6722, grad_fn=<ToCopyBackward0>)\n",
      "21: 48.67216110229492\n",
      "tensor(129.3454, grad_fn=<ToCopyBackward0>)\n",
      "22: 129.3453826904297\n",
      "tensor(106.4402, grad_fn=<ToCopyBackward0>)\n",
      "23: 106.44017791748047\n",
      "tensor(42.3524, grad_fn=<ToCopyBackward0>)\n",
      "24: 42.352352142333984\n",
      "tensor(16.8652, grad_fn=<ToCopyBackward0>)\n",
      "25: 16.865238189697266\n",
      "tensor(139.0035, grad_fn=<ToCopyBackward0>)\n",
      "26: 139.0035400390625\n",
      "tensor(89.2228, grad_fn=<ToCopyBackward0>)\n",
      "27: 89.2227554321289\n",
      "tensor(110.1025, grad_fn=<ToCopyBackward0>)\n",
      "28: 110.10247802734375\n",
      "tensor(40.3285, grad_fn=<ToCopyBackward0>)\n",
      "29: 40.3285026550293\n",
      "tensor(44.6196, grad_fn=<ToCopyBackward0>)\n",
      "30: 44.619564056396484\n",
      "tensor(141.5579, grad_fn=<ToCopyBackward0>)\n",
      "31: 141.5579376220703\n",
      "tensor(194.1407, grad_fn=<ToCopyBackward0>)\n",
      "32: 194.14068603515625\n",
      "tensor(66.3173, grad_fn=<ToCopyBackward0>)\n",
      "33: 66.31727600097656\n",
      "tensor(151.2509, grad_fn=<ToCopyBackward0>)\n",
      "34: 151.25088500976562\n",
      "tensor(165.8345, grad_fn=<ToCopyBackward0>)\n",
      "35: 165.83453369140625\n",
      "tensor(12.8457, grad_fn=<ToCopyBackward0>)\n",
      "36: 12.845730781555176\n",
      "tensor(25.7246, grad_fn=<ToCopyBackward0>)\n",
      "37: 25.724628448486328\n",
      "tensor(108.9480, grad_fn=<ToCopyBackward0>)\n",
      "38: 108.9480209350586\n",
      "tensor(90.0676, grad_fn=<ToCopyBackward0>)\n",
      "39: 90.06756591796875\n",
      "tensor(110.6527, grad_fn=<ToCopyBackward0>)\n",
      "40: 110.65267181396484\n",
      "tensor(22.5506, grad_fn=<ToCopyBackward0>)\n",
      "41: 22.550588607788086\n",
      "tensor(221.2878, grad_fn=<ToCopyBackward0>)\n",
      "42: 221.28775024414062\n",
      "tensor(75.1865, grad_fn=<ToCopyBackward0>)\n",
      "43: 75.1865234375\n",
      "tensor(151.2366, grad_fn=<ToCopyBackward0>)\n",
      "44: 151.2366485595703\n",
      "tensor(37.4289, grad_fn=<ToCopyBackward0>)\n",
      "45: 37.42893600463867\n",
      "tensor(85.7721, grad_fn=<ToCopyBackward0>)\n",
      "46: 85.77208709716797\n",
      "tensor(73.7721, grad_fn=<ToCopyBackward0>)\n",
      "47: 73.77214813232422\n",
      "tensor(78.3077, grad_fn=<ToCopyBackward0>)\n",
      "48: 78.30773162841797\n",
      "tensor(24.8433, grad_fn=<ToCopyBackward0>)\n",
      "49: 24.84326171875\n",
      "tensor(72.6008, grad_fn=<ToCopyBackward0>)\n",
      "50: 72.60083770751953\n",
      "tensor(116.0656, grad_fn=<ToCopyBackward0>)\n",
      "51: 116.06559753417969\n",
      "tensor(27.1364, grad_fn=<ToCopyBackward0>)\n",
      "52: 27.136377334594727\n",
      "tensor(137.5306, grad_fn=<ToCopyBackward0>)\n",
      "53: 137.5306396484375\n",
      "tensor(116.6519, grad_fn=<ToCopyBackward0>)\n",
      "54: 116.65194702148438\n",
      "tensor(184.4098, grad_fn=<ToCopyBackward0>)\n",
      "55: 184.40982055664062\n",
      "tensor(3.6718, grad_fn=<ToCopyBackward0>)\n",
      "56: 3.6717567443847656\n",
      "tensor(140.9452, grad_fn=<ToCopyBackward0>)\n",
      "57: 140.9451904296875\n",
      "tensor(140.3593, grad_fn=<ToCopyBackward0>)\n",
      "58: 140.3592987060547\n",
      "tensor(97.4804, grad_fn=<ToCopyBackward0>)\n",
      "59: 97.48040008544922\n",
      "tensor(225.5820, grad_fn=<ToCopyBackward0>)\n",
      "60: 225.58200073242188\n",
      "tensor(213.2890, grad_fn=<ToCopyBackward0>)\n",
      "61: 213.28895568847656\n",
      "tensor(77.1873, grad_fn=<ToCopyBackward0>)\n",
      "62: 77.18733978271484\n",
      "tensor(163.2380, grad_fn=<ToCopyBackward0>)\n",
      "63: 163.2379913330078\n",
      "tensor(87.4802, grad_fn=<ToCopyBackward0>)\n",
      "64: 87.48015594482422\n",
      "tensor(104.6519, grad_fn=<ToCopyBackward0>)\n",
      "65: 104.65187072753906\n",
      "tensor(84.6012, grad_fn=<ToCopyBackward0>)\n",
      "66: 84.60115051269531\n",
      "tensor(169.8234, grad_fn=<ToCopyBackward0>)\n",
      "67: 169.8234100341797\n",
      "tensor(162.9445, grad_fn=<ToCopyBackward0>)\n",
      "68: 162.94448852539062\n",
      "tensor(196.9949, grad_fn=<ToCopyBackward0>)\n",
      "69: 196.994873046875\n",
      "tensor(114.6513, grad_fn=<ToCopyBackward0>)\n",
      "70: 114.65133666992188\n",
      "tensor(177.5300, grad_fn=<ToCopyBackward0>)\n",
      "71: 177.5299835205078\n",
      "tensor(171.2370, grad_fn=<ToCopyBackward0>)\n",
      "72: 171.23696899414062\n",
      "tensor(103.4794, grad_fn=<ToCopyBackward0>)\n",
      "73: 103.47943115234375\n",
      "tensor(32.0148, grad_fn=<ToCopyBackward0>)\n",
      "74: 32.01484680175781\n",
      "tensor(66.3077, grad_fn=<ToCopyBackward0>)\n",
      "75: 66.3077163696289\n",
      "tensor(66.8935, grad_fn=<ToCopyBackward0>)\n",
      "76: 66.89349365234375\n",
      "tensor(85.1864, grad_fn=<ToCopyBackward0>)\n",
      "77: 85.18636322021484\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "78: 91.18634033203125\n",
      "tensor(88.3579, grad_fn=<ToCopyBackward0>)\n",
      "79: 88.35790252685547\n",
      "tensor(56.8934, grad_fn=<ToCopyBackward0>)\n",
      "80: 56.89341354370117\n",
      "tensor(23.1360, grad_fn=<ToCopyBackward0>)\n",
      "81: 23.136043548583984\n",
      "tensor(43.4289, grad_fn=<ToCopyBackward0>)\n",
      "82: 43.42893600463867\n",
      "tensor(156.1152, grad_fn=<ToCopyBackward0>)\n",
      "83: 156.11521911621094\n",
      "tensor(134.6508, grad_fn=<ToCopyBackward0>)\n",
      "84: 134.6507568359375\n",
      "tensor(73.7721, grad_fn=<ToCopyBackward0>)\n",
      "85: 73.77207946777344\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "86: 91.18629455566406\n",
      "tensor(84.8934, grad_fn=<ToCopyBackward0>)\n",
      "87: 84.89340209960938\n",
      "tensor(194.7010, grad_fn=<ToCopyBackward0>)\n",
      "88: 194.70101928710938\n",
      "tensor(172.1152, grad_fn=<ToCopyBackward0>)\n",
      "89: 172.115234375\n",
      "tensor(182.7010, grad_fn=<ToCopyBackward0>)\n",
      "90: 182.70103454589844\n",
      "tensor(88.8934, grad_fn=<ToCopyBackward0>)\n",
      "91: 88.8934097290039\n",
      "tensor(126.6508, grad_fn=<ToCopyBackward0>)\n",
      "92: 126.6507797241211\n",
      "tensor(59.1863, grad_fn=<ToCopyBackward0>)\n",
      "93: 59.186309814453125\n",
      "tensor(81.7721, grad_fn=<ToCopyBackward0>)\n",
      "94: 81.77210235595703\n",
      "tensor(28.8432, grad_fn=<ToCopyBackward0>)\n",
      "95: 28.843154907226562\n",
      "tensor(145.8224, grad_fn=<ToCopyBackward0>)\n",
      "96: 145.82237243652344\n",
      "tensor(99.1863, grad_fn=<ToCopyBackward0>)\n",
      "97: 99.18631744384766\n",
      "tensor(66.8934, grad_fn=<ToCopyBackward0>)\n",
      "98: 66.89341735839844\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "99: 128.65078735351562\n",
      "tensor(136.3579, grad_fn=<ToCopyBackward0>)\n",
      "100: 136.35789489746094\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "101: 71.18630981445312\n",
      "tensor(94.0650, grad_fn=<ToCopyBackward0>)\n",
      "102: 94.06499481201172\n",
      "tensor(140.6508, grad_fn=<ToCopyBackward0>)\n",
      "103: 140.65077209472656\n",
      "tensor(46.3076, grad_fn=<ToCopyBackward0>)\n",
      "104: 46.307621002197266\n",
      "tensor(95.4792, grad_fn=<ToCopyBackward0>)\n",
      "105: 95.47919464111328\n",
      "tensor(83.1863, grad_fn=<ToCopyBackward0>)\n",
      "106: 83.1863021850586\n",
      "tensor(153.8223, grad_fn=<ToCopyBackward0>)\n",
      "107: 153.8223419189453\n",
      "tensor(7.9645, grad_fn=<ToCopyBackward0>)\n",
      "108: 7.964468002319336\n",
      "tensor(95.4792, grad_fn=<ToCopyBackward0>)\n",
      "109: 95.47918701171875\n",
      "tensor(132.9437, grad_fn=<ToCopyBackward0>)\n",
      "110: 132.94366455078125\n",
      "tensor(59.4289, grad_fn=<ToCopyBackward0>)\n",
      "111: 59.42893600463867\n",
      "tensor(129.0983, grad_fn=<ToCopyBackward0>)\n",
      "112: 129.09825134277344\n",
      "tensor(88.8934, grad_fn=<ToCopyBackward0>)\n",
      "113: 88.89340209960938\n",
      "tensor(137.5294, grad_fn=<ToCopyBackward0>)\n",
      "114: 137.52943420410156\n",
      "tensor(133.7721, grad_fn=<ToCopyBackward0>)\n",
      "115: 133.77207946777344\n",
      "tensor(97.4792, grad_fn=<ToCopyBackward0>)\n",
      "116: 97.47918701171875\n",
      "tensor(133.7721, grad_fn=<ToCopyBackward0>)\n",
      "117: 133.77207946777344\n",
      "tensor(120.3579, grad_fn=<ToCopyBackward0>)\n",
      "118: 120.35786437988281\n",
      "tensor(149.5294, grad_fn=<ToCopyBackward0>)\n",
      "119: 149.52943420410156\n",
      "tensor(87.7721, grad_fn=<ToCopyBackward0>)\n",
      "120: 87.77207946777344\n",
      "tensor(29.4289, grad_fn=<ToCopyBackward0>)\n",
      "121: 29.428932189941406\n",
      "tensor(27.4289, grad_fn=<ToCopyBackward0>)\n",
      "122: 27.428932189941406\n",
      "tensor(157.5294, grad_fn=<ToCopyBackward0>)\n",
      "123: 157.52943420410156\n",
      "tensor(42.0147, grad_fn=<ToCopyBackward0>)\n",
      "124: 42.01472091674805\n",
      "tensor(10.2574, grad_fn=<ToCopyBackward0>)\n",
      "125: 10.257360458374023\n",
      "tensor(7.6716, grad_fn=<ToCopyBackward0>)\n",
      "126: 7.671573638916016\n",
      "tensor(114.6508, grad_fn=<ToCopyBackward0>)\n",
      "127: 114.65076446533203\n",
      "tensor(132.9436, grad_fn=<ToCopyBackward0>)\n",
      "128: 132.9436492919922\n",
      "tensor(172.1152, grad_fn=<ToCopyBackward0>)\n",
      "129: 172.115234375\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "130: 71.18629455566406\n",
      "tensor(45.4289, grad_fn=<ToCopyBackward0>)\n",
      "131: 45.428932189941406\n",
      "tensor(104.3579, grad_fn=<ToCopyBackward0>)\n",
      "132: 104.35786437988281\n",
      "tensor(88.8934, grad_fn=<ToCopyBackward0>)\n",
      "133: 88.89340209960938\n",
      "tensor(194.9939, grad_fn=<ToCopyBackward0>)\n",
      "134: 194.99391174316406\n",
      "tensor(116.3579, grad_fn=<ToCopyBackward0>)\n",
      "135: 116.35786437988281\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "136: 71.18629455566406\n",
      "tensor(198.7010, grad_fn=<ToCopyBackward0>)\n",
      "137: 198.70101928710938\n",
      "tensor(84.8934, grad_fn=<ToCopyBackward0>)\n",
      "138: 84.89340209960938\n",
      "tensor(110.6508, grad_fn=<ToCopyBackward0>)\n",
      "139: 110.6507568359375\n",
      "tensor(106.6508, grad_fn=<ToCopyBackward0>)\n",
      "140: 106.6507568359375\n",
      "tensor(104.6508, grad_fn=<ToCopyBackward0>)\n",
      "141: 104.6507568359375\n",
      "tensor(50.6005, grad_fn=<ToCopyBackward0>)\n",
      "142: 50.60050582885742\n",
      "tensor(160.1152, grad_fn=<ToCopyBackward0>)\n",
      "143: 160.11521911621094\n",
      "tensor(151.5294, grad_fn=<ToCopyBackward0>)\n",
      "144: 151.52943420410156\n",
      "tensor(126.6508, grad_fn=<ToCopyBackward0>)\n",
      "145: 126.6507568359375\n",
      "tensor(141.2365, grad_fn=<ToCopyBackward0>)\n",
      "146: 141.23654174804688\n",
      "tensor(116.6508, grad_fn=<ToCopyBackward0>)\n",
      "147: 116.6507568359375\n",
      "tensor(78.8934, grad_fn=<ToCopyBackward0>)\n",
      "148: 78.89339447021484\n",
      "tensor(109.7721, grad_fn=<ToCopyBackward0>)\n",
      "149: 109.77207946777344\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "150: 71.18629455566406\n",
      "tensor(35.4289, grad_fn=<ToCopyBackward0>)\n",
      "151: 35.428932189941406\n",
      "tensor(74.6005, grad_fn=<ToCopyBackward0>)\n",
      "152: 74.60050201416016\n",
      "tensor(45.4289, grad_fn=<ToCopyBackward0>)\n",
      "153: 45.428932189941406\n",
      "tensor(96.3579, grad_fn=<ToCopyBackward0>)\n",
      "154: 96.35786437988281\n",
      "tensor(132.6508, grad_fn=<ToCopyBackward0>)\n",
      "155: 132.6507568359375\n",
      "tensor(97.7535, grad_fn=<ToCopyBackward0>)\n",
      "156: 97.75353240966797\n",
      "tensor(126.9436, grad_fn=<ToCopyBackward0>)\n",
      "157: 126.94364929199219\n",
      "tensor(27.4289, grad_fn=<ToCopyBackward0>)\n",
      "158: 27.428932189941406\n",
      "tensor(43.7218, grad_fn=<ToCopyBackward0>)\n",
      "159: 43.721824645996094\n",
      "tensor(90.0650, grad_fn=<ToCopyBackward0>)\n",
      "160: 90.06497192382812\n",
      "tensor(134.6508, grad_fn=<ToCopyBackward0>)\n",
      "161: 134.6507568359375\n",
      "tensor(88.6005, grad_fn=<ToCopyBackward0>)\n",
      "162: 88.60050201416016\n",
      "tensor(122.9436, grad_fn=<ToCopyBackward0>)\n",
      "163: 122.94364929199219\n",
      "tensor(98.3579, grad_fn=<ToCopyBackward0>)\n",
      "164: 98.35786437988281\n",
      "tensor(100.6508, grad_fn=<ToCopyBackward0>)\n",
      "165: 100.6507568359375\n",
      "tensor(194.1152, grad_fn=<ToCopyBackward0>)\n",
      "166: 194.11521911621094\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "167: 128.6507568359375\n",
      "tensor(87.4792, grad_fn=<ToCopyBackward0>)\n",
      "168: 87.47918701171875\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "169: 128.6507568359375\n",
      "tensor(99.4792, grad_fn=<ToCopyBackward0>)\n",
      "170: 99.47918701171875\n",
      "tensor(110.3579, grad_fn=<ToCopyBackward0>)\n",
      "171: 110.35786437988281\n",
      "tensor(79.7721, grad_fn=<ToCopyBackward0>)\n",
      "172: 79.77207946777344\n",
      "tensor(172.1152, grad_fn=<ToCopyBackward0>)\n",
      "173: 172.11521911621094\n",
      "tensor(147.8223, grad_fn=<ToCopyBackward0>)\n",
      "174: 147.82232666015625\n",
      "tensor(77.7218, grad_fn=<ToCopyBackward0>)\n",
      "175: 77.7218246459961\n",
      "tensor(61.4289, grad_fn=<ToCopyBackward0>)\n",
      "176: 61.428932189941406\n",
      "tensor(160.1152, grad_fn=<ToCopyBackward0>)\n",
      "177: 160.11521911621094\n",
      "tensor(101.7721, grad_fn=<ToCopyBackward0>)\n",
      "178: 101.77207946777344\n",
      "tensor(112.0650, grad_fn=<ToCopyBackward0>)\n",
      "179: 112.06497192382812\n",
      "tensor(35.1360, grad_fn=<ToCopyBackward0>)\n",
      "180: 35.13603973388672\n",
      "tensor(215.2868, grad_fn=<ToCopyBackward0>)\n",
      "181: 215.28680419921875\n",
      "tensor(184.4081, grad_fn=<ToCopyBackward0>)\n",
      "182: 184.40811157226562\n",
      "tensor(145.5294, grad_fn=<ToCopyBackward0>)\n",
      "183: 145.52943420410156\n",
      "tensor(172.1152, grad_fn=<ToCopyBackward0>)\n",
      "184: 172.11521911621094\n",
      "tensor(127.2365, grad_fn=<ToCopyBackward0>)\n",
      "185: 127.23654174804688\n",
      "tensor(84.0650, grad_fn=<ToCopyBackward0>)\n",
      "186: 84.06497192382812\n",
      "tensor(141.5294, grad_fn=<ToCopyBackward0>)\n",
      "187: 141.52943420410156\n",
      "tensor(114.6508, grad_fn=<ToCopyBackward0>)\n",
      "188: 114.6507568359375\n",
      "tensor(24.8431, grad_fn=<ToCopyBackward0>)\n",
      "189: 24.8431453704834\n",
      "tensor(100.3579, grad_fn=<ToCopyBackward0>)\n",
      "190: 100.35786437988281\n",
      "tensor(22.2574, grad_fn=<ToCopyBackward0>)\n",
      "191: 22.25735855102539\n",
      "tensor(29.4289, grad_fn=<ToCopyBackward0>)\n",
      "192: 29.428932189941406\n",
      "tensor(44.6005, grad_fn=<ToCopyBackward0>)\n",
      "193: 44.60050582885742\n",
      "tensor(54.0147, grad_fn=<ToCopyBackward0>)\n",
      "194: 54.01472091674805\n",
      "tensor(282.7513, grad_fn=<ToCopyBackward0>)\n",
      "195: 282.7512512207031\n",
      "tensor(82.3076, grad_fn=<ToCopyBackward0>)\n",
      "196: 82.30760955810547\n",
      "tensor(104.3579, grad_fn=<ToCopyBackward0>)\n",
      "197: 104.35786437988281\n",
      "tensor(119.4792, grad_fn=<ToCopyBackward0>)\n",
      "198: 119.47918701171875\n",
      "tensor(68.3076, grad_fn=<ToCopyBackward0>)\n",
      "199: 68.30760955810547\n",
      "tensor(85.4792, grad_fn=<ToCopyBackward0>)\n",
      "200: 85.47918701171875\n",
      "tensor(72.3076, grad_fn=<ToCopyBackward0>)\n",
      "201: 72.30760955810547\n",
      "tensor(135.5294, grad_fn=<ToCopyBackward0>)\n",
      "202: 135.52943420410156\n",
      "tensor(145.5294, grad_fn=<ToCopyBackward0>)\n",
      "203: 145.52943420410156\n",
      "tensor(3.6716, grad_fn=<ToCopyBackward0>)\n",
      "204: 3.6715729236602783\n",
      "tensor(142.9436, grad_fn=<ToCopyBackward0>)\n",
      "205: 142.9436492919922\n",
      "tensor(108.3579, grad_fn=<ToCopyBackward0>)\n",
      "206: 108.35786437988281\n",
      "tensor(184.4081, grad_fn=<ToCopyBackward0>)\n",
      "207: 184.40811157226562\n",
      "tensor(31.4289, grad_fn=<ToCopyBackward0>)\n",
      "208: 31.428932189941406\n",
      "tensor(52.6005, grad_fn=<ToCopyBackward0>)\n",
      "209: 52.60050582885742\n",
      "tensor(83.1863, grad_fn=<ToCopyBackward0>)\n",
      "210: 83.18629455566406\n",
      "tensor(215.2868, grad_fn=<ToCopyBackward0>)\n",
      "211: 215.28680419921875\n",
      "tensor(84.0650, grad_fn=<ToCopyBackward0>)\n",
      "212: 84.06497192382812\n",
      "tensor(66.0147, grad_fn=<ToCopyBackward0>)\n",
      "213: 66.01471710205078\n",
      "tensor(42.6005, grad_fn=<ToCopyBackward0>)\n",
      "214: 42.60050582885742\n",
      "tensor(29.7218, grad_fn=<ToCopyBackward0>)\n",
      "215: 29.721824645996094\n",
      "tensor(46.0147, grad_fn=<ToCopyBackward0>)\n",
      "216: 46.01471710205078\n",
      "tensor(66.0147, grad_fn=<ToCopyBackward0>)\n",
      "217: 66.01471710205078\n",
      "tensor(76.2946, grad_fn=<ToCopyBackward0>)\n",
      "218: 76.29463195800781\n",
      "tensor(56.8934, grad_fn=<ToCopyBackward0>)\n",
      "219: 56.89339828491211\n",
      "tensor(89.4792, grad_fn=<ToCopyBackward0>)\n",
      "220: 89.47918701171875\n",
      "tensor(129.2365, grad_fn=<ToCopyBackward0>)\n",
      "221: 129.23654174804688\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "222: 102.06497192382812\n",
      "tensor(181.8223, grad_fn=<ToCopyBackward0>)\n",
      "223: 181.82232666015625\n",
      "tensor(23.1360, grad_fn=<ToCopyBackward0>)\n",
      "224: 23.13603973388672\n",
      "tensor(48.0147, grad_fn=<ToCopyBackward0>)\n",
      "225: 48.01471710205078\n",
      "tensor(9.9645, grad_fn=<ToCopyBackward0>)\n",
      "226: 9.964466094970703\n",
      "tensor(186.7010, grad_fn=<ToCopyBackward0>)\n",
      "227: 186.7010040283203\n",
      "tensor(84.6005, grad_fn=<ToCopyBackward0>)\n",
      "228: 84.60050201416016\n",
      "tensor(12.5503, grad_fn=<ToCopyBackward0>)\n",
      "229: 12.550251960754395\n",
      "tensor(154.9436, grad_fn=<ToCopyBackward0>)\n",
      "230: 154.9436492919922\n",
      "tensor(122.0650, grad_fn=<ToCopyBackward0>)\n",
      "231: 122.06497192382812\n",
      "tensor(21.4289, grad_fn=<ToCopyBackward0>)\n",
      "232: 21.428932189941406\n",
      "tensor(129.2365, grad_fn=<ToCopyBackward0>)\n",
      "233: 129.23654174804688\n",
      "tensor(116.9436, grad_fn=<ToCopyBackward0>)\n",
      "234: 116.94364929199219\n",
      "tensor(75.4289, grad_fn=<ToCopyBackward0>)\n",
      "235: 75.4289321899414\n",
      "tensor(64.8934, grad_fn=<ToCopyBackward0>)\n",
      "236: 64.89340209960938\n",
      "tensor(122.0650, grad_fn=<ToCopyBackward0>)\n",
      "237: 122.06497192382812\n",
      "tensor(177.5294, grad_fn=<ToCopyBackward0>)\n",
      "238: 177.52943420410156\n",
      "tensor(27.4289, grad_fn=<ToCopyBackward0>)\n",
      "239: 27.428932189941406\n",
      "tensor(58.2946, grad_fn=<ToCopyBackward0>)\n",
      "240: 58.294612884521484\n",
      "tensor(80.6005, grad_fn=<ToCopyBackward0>)\n",
      "241: 80.60050201416016\n",
      "tensor(176.4081, grad_fn=<ToCopyBackward0>)\n",
      "242: 176.40811157226562\n",
      "tensor(10.2574, grad_fn=<ToCopyBackward0>)\n",
      "243: 10.257359504699707\n",
      "tensor(162.1152, grad_fn=<ToCopyBackward0>)\n",
      "244: 162.11521911621094\n",
      "tensor(76.6005, grad_fn=<ToCopyBackward0>)\n",
      "245: 76.60050201416016\n",
      "tensor(105.7721, grad_fn=<ToCopyBackward0>)\n",
      "246: 105.77207946777344\n",
      "tensor(68.8934, grad_fn=<ToCopyBackward0>)\n",
      "247: 68.89339447021484\n",
      "tensor(67.1863, grad_fn=<ToCopyBackward0>)\n",
      "248: 67.18629455566406\n",
      "tensor(160.1152, grad_fn=<ToCopyBackward0>)\n",
      "249: 160.11521911621094\n",
      "tensor(78.8934, grad_fn=<ToCopyBackward0>)\n",
      "250: 78.89340209960938\n",
      "tensor(75.4289, grad_fn=<ToCopyBackward0>)\n",
      "251: 75.4289321899414\n",
      "tensor(48.0147, grad_fn=<ToCopyBackward0>)\n",
      "252: 48.01471710205078\n",
      "tensor(53.7218, grad_fn=<ToCopyBackward0>)\n",
      "253: 53.721824645996094\n",
      "tensor(85.4792, grad_fn=<ToCopyBackward0>)\n",
      "254: 85.47918701171875\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "255: 91.18629455566406\n",
      "tensor(193.8223, grad_fn=<ToCopyBackward0>)\n",
      "256: 193.82232666015625\n",
      "tensor(100.6508, grad_fn=<ToCopyBackward0>)\n",
      "257: 100.6507568359375\n",
      "tensor(110.6508, grad_fn=<ToCopyBackward0>)\n",
      "258: 110.6507568359375\n",
      "tensor(122.9436, grad_fn=<ToCopyBackward0>)\n",
      "259: 122.94364929199219\n",
      "tensor(63.4792, grad_fn=<ToCopyBackward0>)\n",
      "260: 63.479183197021484\n",
      "tensor(110.0650, grad_fn=<ToCopyBackward0>)\n",
      "261: 110.06497192382812\n",
      "tensor(209.2868, grad_fn=<ToCopyBackward0>)\n",
      "262: 209.28680419921875\n",
      "tensor(136.0650, grad_fn=<ToCopyBackward0>)\n",
      "263: 136.06497192382812\n",
      "tensor(87.4792, grad_fn=<ToCopyBackward0>)\n",
      "264: 87.47918701171875\n",
      "tensor(64.0147, grad_fn=<ToCopyBackward0>)\n",
      "265: 64.01471710205078\n",
      "tensor(82.0147, grad_fn=<ToCopyBackward0>)\n",
      "266: 82.01471710205078\n",
      "tensor(141.5294, grad_fn=<ToCopyBackward0>)\n",
      "267: 141.52943420410156\n",
      "tensor(152.6508, grad_fn=<ToCopyBackward0>)\n",
      "268: 152.6507568359375\n",
      "tensor(181.8223, grad_fn=<ToCopyBackward0>)\n",
      "269: 181.82232666015625\n",
      "tensor(47.4289, grad_fn=<ToCopyBackward0>)\n",
      "270: 47.428932189941406\n",
      "tensor(52.3076, grad_fn=<ToCopyBackward0>)\n",
      "271: 52.307613372802734\n",
      "tensor(87.7721, grad_fn=<ToCopyBackward0>)\n",
      "272: 87.77207946777344\n",
      "tensor(102.6508, grad_fn=<ToCopyBackward0>)\n",
      "273: 102.6507568359375\n",
      "tensor(82.0147, grad_fn=<ToCopyBackward0>)\n",
      "274: 82.01471710205078\n",
      "tensor(184.1152, grad_fn=<ToCopyBackward0>)\n",
      "275: 184.11521911621094\n",
      "tensor(204.9939, grad_fn=<ToCopyBackward0>)\n",
      "276: 204.993896484375\n",
      "tensor(121.2365, grad_fn=<ToCopyBackward0>)\n",
      "277: 121.23654174804688\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "278: 112.35786437988281\n",
      "tensor(193.8223, grad_fn=<ToCopyBackward0>)\n",
      "279: 193.82232666015625\n",
      "tensor(116.9436, grad_fn=<ToCopyBackward0>)\n",
      "280: 116.94364929199219\n",
      "tensor(87.4792, grad_fn=<ToCopyBackward0>)\n",
      "281: 87.47918701171875\n",
      "tensor(23.1360, grad_fn=<ToCopyBackward0>)\n",
      "282: 23.13603973388672\n",
      "tensor(3.6716, grad_fn=<ToCopyBackward0>)\n",
      "283: 3.6715729236602783\n",
      "tensor(93.4792, grad_fn=<ToCopyBackward0>)\n",
      "284: 93.47918701171875\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "285: 112.35786437988281\n",
      "tensor(136.0650, grad_fn=<ToCopyBackward0>)\n",
      "286: 136.06497192382812\n",
      "tensor(71.4792, grad_fn=<ToCopyBackward0>)\n",
      "287: 71.47918701171875\n",
      "tensor(149.2365, grad_fn=<ToCopyBackward0>)\n",
      "288: 149.23654174804688\n",
      "tensor(58.0147, grad_fn=<ToCopyBackward0>)\n",
      "289: 58.01471710205078\n",
      "tensor(35.1360, grad_fn=<ToCopyBackward0>)\n",
      "290: 35.13603973388672\n",
      "tensor(138.6508, grad_fn=<ToCopyBackward0>)\n",
      "291: 138.6507568359375\n",
      "tensor(85.1863, grad_fn=<ToCopyBackward0>)\n",
      "292: 85.18629455566406\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "293: 112.35786437988281\n",
      "tensor(137.2365, grad_fn=<ToCopyBackward0>)\n",
      "294: 137.23654174804688\n",
      "tensor(208.4081, grad_fn=<ToCopyBackward0>)\n",
      "295: 208.40811157226562\n",
      "tensor(83.1863, grad_fn=<ToCopyBackward0>)\n",
      "296: 83.18629455566406\n",
      "tensor(59.1863, grad_fn=<ToCopyBackward0>)\n",
      "297: 59.1862907409668\n",
      "tensor(178.1152, grad_fn=<ToCopyBackward0>)\n",
      "298: 178.11521911621094\n",
      "tensor(58.3076, grad_fn=<ToCopyBackward0>)\n",
      "299: 58.307613372802734\n",
      "tensor(35.1360, grad_fn=<ToCopyBackward0>)\n",
      "300: 35.13603973388672\n",
      "tensor(163.5294, grad_fn=<ToCopyBackward0>)\n",
      "301: 163.52943420410156\n",
      "tensor(79.1863, grad_fn=<ToCopyBackward0>)\n",
      "302: 79.18629455566406\n",
      "tensor(109.7721, grad_fn=<ToCopyBackward0>)\n",
      "303: 109.77207946777344\n",
      "tensor(181.5294, grad_fn=<ToCopyBackward0>)\n",
      "304: 181.52943420410156\n",
      "tensor(47.1360, grad_fn=<ToCopyBackward0>)\n",
      "305: 47.13603973388672\n",
      "tensor(30.0147, grad_fn=<ToCopyBackward0>)\n",
      "306: 30.014719009399414\n",
      "tensor(89.1863, grad_fn=<ToCopyBackward0>)\n",
      "307: 89.18629455566406\n",
      "tensor(101.4792, grad_fn=<ToCopyBackward0>)\n",
      "308: 101.47918701171875\n",
      "tensor(97.7721, grad_fn=<ToCopyBackward0>)\n",
      "309: 97.77207946777344\n",
      "tensor(62.0147, grad_fn=<ToCopyBackward0>)\n",
      "310: 62.01471710205078\n",
      "tensor(82.3076, grad_fn=<ToCopyBackward0>)\n",
      "311: 82.30760955810547\n",
      "tensor(62.0147, grad_fn=<ToCopyBackward0>)\n",
      "312: 62.01471710205078\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "313: 91.18629455566406\n",
      "tensor(143.2365, grad_fn=<ToCopyBackward0>)\n",
      "314: 143.23654174804688\n",
      "tensor(66.8934, grad_fn=<ToCopyBackward0>)\n",
      "315: 66.89340209960938\n",
      "tensor(147.5294, grad_fn=<ToCopyBackward0>)\n",
      "316: 147.52943420410156\n",
      "tensor(151.2365, grad_fn=<ToCopyBackward0>)\n",
      "317: 151.23654174804688\n",
      "tensor(43.7218, grad_fn=<ToCopyBackward0>)\n",
      "318: 43.721824645996094\n",
      "tensor(266.4584, grad_fn=<ToCopyBackward0>)\n",
      "319: 266.4583740234375\n",
      "tensor(178.1152, grad_fn=<ToCopyBackward0>)\n",
      "320: 178.11521911621094\n",
      "tensor(109.7721, grad_fn=<ToCopyBackward0>)\n",
      "321: 109.77207946777344\n",
      "tensor(147.5294, grad_fn=<ToCopyBackward0>)\n",
      "322: 147.52943420410156\n",
      "tensor(181.8223, grad_fn=<ToCopyBackward0>)\n",
      "323: 181.82232666015625\n",
      "tensor(126.3579, grad_fn=<ToCopyBackward0>)\n",
      "324: 126.35786437988281\n",
      "tensor(94.3579, grad_fn=<ToCopyBackward0>)\n",
      "325: 94.35786437988281\n",
      "tensor(73.1863, grad_fn=<ToCopyBackward0>)\n",
      "326: 73.18629455566406\n",
      "tensor(66.6005, grad_fn=<ToCopyBackward0>)\n",
      "327: 66.60050201416016\n",
      "tensor(36.0147, grad_fn=<ToCopyBackward0>)\n",
      "328: 36.01471710205078\n",
      "tensor(35.4289, grad_fn=<ToCopyBackward0>)\n",
      "329: 35.428932189941406\n",
      "tensor(50.0147, grad_fn=<ToCopyBackward0>)\n",
      "330: 50.01471710205078\n",
      "tensor(92.0650, grad_fn=<ToCopyBackward0>)\n",
      "331: 92.06497192382812\n",
      "tensor(37.7218, grad_fn=<ToCopyBackward0>)\n",
      "332: 37.721824645996094\n",
      "tensor(29.7218, grad_fn=<ToCopyBackward0>)\n",
      "333: 29.721824645996094\n",
      "tensor(78.3076, grad_fn=<ToCopyBackward0>)\n",
      "334: 78.30760955810547\n",
      "tensor(85.3938, grad_fn=<ToCopyBackward0>)\n",
      "335: 85.39383697509766\n",
      "tensor(156.1152, grad_fn=<ToCopyBackward0>)\n",
      "336: 156.11521911621094\n",
      "tensor(93.4792, grad_fn=<ToCopyBackward0>)\n",
      "337: 93.47918701171875\n",
      "tensor(192.9939, grad_fn=<ToCopyBackward0>)\n",
      "338: 192.993896484375\n",
      "tensor(76.6005, grad_fn=<ToCopyBackward0>)\n",
      "339: 76.60050201416016\n",
      "tensor(82.3076, grad_fn=<ToCopyBackward0>)\n",
      "340: 82.30760955810547\n",
      "tensor(54.0147, grad_fn=<ToCopyBackward0>)\n",
      "341: 54.01472091674805\n",
      "tensor(39.1360, grad_fn=<ToCopyBackward0>)\n",
      "342: 39.13603973388672\n",
      "tensor(58.0147, grad_fn=<ToCopyBackward0>)\n",
      "343: 58.01471710205078\n",
      "tensor(118.3579, grad_fn=<ToCopyBackward0>)\n",
      "344: 118.35786437988281\n",
      "tensor(112.6508, grad_fn=<ToCopyBackward0>)\n",
      "345: 112.6507568359375\n",
      "tensor(85.1863, grad_fn=<ToCopyBackward0>)\n",
      "346: 85.18629455566406\n",
      "tensor(136.0451, grad_fn=<ToCopyBackward0>)\n",
      "347: 136.04510498046875\n",
      "tensor(66.8934, grad_fn=<ToCopyBackward0>)\n",
      "348: 66.89340209960938\n",
      "tensor(60.3076, grad_fn=<ToCopyBackward0>)\n",
      "349: 60.307613372802734\n",
      "tensor(177.5294, grad_fn=<ToCopyBackward0>)\n",
      "350: 177.52943420410156\n",
      "tensor(23.4289, grad_fn=<ToCopyBackward0>)\n",
      "351: 23.428932189941406\n",
      "tensor(64.3076, grad_fn=<ToCopyBackward0>)\n",
      "352: 64.30760955810547\n",
      "tensor(42.3076, grad_fn=<ToCopyBackward0>)\n",
      "353: 42.307613372802734\n",
      "tensor(71.4792, grad_fn=<ToCopyBackward0>)\n",
      "354: 71.47918701171875\n",
      "tensor(162.1152, grad_fn=<ToCopyBackward0>)\n",
      "355: 162.11521911621094\n",
      "tensor(116.3579, grad_fn=<ToCopyBackward0>)\n",
      "356: 116.35786437988281\n",
      "tensor(42.0147, grad_fn=<ToCopyBackward0>)\n",
      "357: 42.01471710205078\n",
      "tensor(96.0650, grad_fn=<ToCopyBackward0>)\n",
      "358: 96.06497192382812\n",
      "tensor(73.1863, grad_fn=<ToCopyBackward0>)\n",
      "359: 73.18629455566406\n",
      "tensor(176.4081, grad_fn=<ToCopyBackward0>)\n",
      "360: 176.40811157226562\n",
      "tensor(196.9939, grad_fn=<ToCopyBackward0>)\n",
      "361: 196.993896484375\n",
      "tensor(110.9436, grad_fn=<ToCopyBackward0>)\n",
      "362: 110.94364929199219\n",
      "tensor(147.5294, grad_fn=<ToCopyBackward0>)\n",
      "363: 147.52943420410156\n",
      "tensor(137.2365, grad_fn=<ToCopyBackward0>)\n",
      "364: 137.23654174804688\n",
      "tensor(138.6508, grad_fn=<ToCopyBackward0>)\n",
      "365: 138.6507568359375\n",
      "tensor(70.8934, grad_fn=<ToCopyBackward0>)\n",
      "366: 70.89340209960938\n",
      "tensor(87.4792, grad_fn=<ToCopyBackward0>)\n",
      "367: 87.47918701171875\n",
      "tensor(82.0147, grad_fn=<ToCopyBackward0>)\n",
      "368: 82.01471710205078\n",
      "tensor(129.5294, grad_fn=<ToCopyBackward0>)\n",
      "369: 129.52943420410156\n",
      "tensor(90.0650, grad_fn=<ToCopyBackward0>)\n",
      "370: 90.06497192382812\n",
      "tensor(93.7721, grad_fn=<ToCopyBackward0>)\n",
      "371: 93.77207946777344\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "372: 102.06497192382812\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "373: 128.6507568359375\n",
      "tensor(155.2365, grad_fn=<ToCopyBackward0>)\n",
      "374: 155.23654174804688\n",
      "tensor(155.8223, grad_fn=<ToCopyBackward0>)\n",
      "375: 155.82232666015625\n",
      "tensor(123.2365, grad_fn=<ToCopyBackward0>)\n",
      "376: 123.23654174804688\n",
      "tensor(153.5294, grad_fn=<ToCopyBackward0>)\n",
      "377: 153.52943420410156\n",
      "tensor(10.5503, grad_fn=<ToCopyBackward0>)\n",
      "378: 10.550252914428711\n",
      "tensor(146.9436, grad_fn=<ToCopyBackward0>)\n",
      "379: 146.9436492919922\n",
      "tensor(93.4792, grad_fn=<ToCopyBackward0>)\n",
      "380: 93.47918701171875\n",
      "tensor(66.2945, grad_fn=<ToCopyBackward0>)\n",
      "381: 66.29448699951172\n",
      "tensor(20.5503, grad_fn=<ToCopyBackward0>)\n",
      "382: 20.55025291442871\n",
      "tensor(146.6068, grad_fn=<ToCopyBackward0>)\n",
      "383: 146.6068115234375\n",
      "tensor(69.4792, grad_fn=<ToCopyBackward0>)\n",
      "384: 69.47918701171875\n",
      "tensor(118.3579, grad_fn=<ToCopyBackward0>)\n",
      "385: 118.35786437988281\n",
      "tensor(47.1360, grad_fn=<ToCopyBackward0>)\n",
      "386: 47.13603973388672\n",
      "tensor(124.0650, grad_fn=<ToCopyBackward0>)\n",
      "387: 124.06497192382812\n",
      "tensor(91.4792, grad_fn=<ToCopyBackward0>)\n",
      "388: 91.47918701171875\n",
      "tensor(172.1152, grad_fn=<ToCopyBackward0>)\n",
      "389: 172.11521911621094\n",
      "tensor(84.6005, grad_fn=<ToCopyBackward0>)\n",
      "390: 84.60050201416016\n",
      "tensor(67.1863, grad_fn=<ToCopyBackward0>)\n",
      "391: 67.18629455566406\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "392: 56.60050582885742\n",
      "tensor(137.0341, grad_fn=<ToCopyBackward0>)\n",
      "393: 137.03407287597656\n",
      "tensor(167.8223, grad_fn=<ToCopyBackward0>)\n",
      "394: 167.82232666015625\n",
      "tensor(76.4330, grad_fn=<ToCopyBackward0>)\n",
      "395: 76.43302154541016\n",
      "tensor(95.4792, grad_fn=<ToCopyBackward0>)\n",
      "396: 95.47918701171875\n",
      "tensor(120.6508, grad_fn=<ToCopyBackward0>)\n",
      "397: 120.6507568359375\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "398: 128.6507568359375\n",
      "tensor(138.6508, grad_fn=<ToCopyBackward0>)\n",
      "399: 138.6507568359375\n",
      "tensor(114.9437, grad_fn=<ToCopyBackward0>)\n",
      "400: 114.94365692138672\n",
      "tensor(72.8934, grad_fn=<ToCopyBackward0>)\n",
      "401: 72.89340209960938\n",
      "tensor(160.1152, grad_fn=<ToCopyBackward0>)\n",
      "402: 160.115234375\n",
      "tensor(27.1360, grad_fn=<ToCopyBackward0>)\n",
      "403: 27.136043548583984\n",
      "tensor(66.3076, grad_fn=<ToCopyBackward0>)\n",
      "404: 66.3076171875\n",
      "tensor(98.0650, grad_fn=<ToCopyBackward0>)\n",
      "405: 98.06497955322266\n",
      "tensor(147.5294, grad_fn=<ToCopyBackward0>)\n",
      "406: 147.52944946289062\n",
      "tensor(50.3076, grad_fn=<ToCopyBackward0>)\n",
      "407: 50.3076171875\n",
      "tensor(110.6508, grad_fn=<ToCopyBackward0>)\n",
      "408: 110.65076446533203\n",
      "tensor(168.1152, grad_fn=<ToCopyBackward0>)\n",
      "409: 168.115234375\n",
      "tensor(153.8223, grad_fn=<ToCopyBackward0>)\n",
      "410: 153.82232666015625\n",
      "tensor(85.4792, grad_fn=<ToCopyBackward0>)\n",
      "411: 85.47918701171875\n",
      "tensor(145.5294, grad_fn=<ToCopyBackward0>)\n",
      "412: 145.52943420410156\n",
      "tensor(135.2365, grad_fn=<ToCopyBackward0>)\n",
      "413: 135.23654174804688\n",
      "tensor(79.7721, grad_fn=<ToCopyBackward0>)\n",
      "414: 79.77207946777344\n",
      "tensor(59.4289, grad_fn=<ToCopyBackward0>)\n",
      "415: 59.428932189941406\n",
      "tensor(176.1152, grad_fn=<ToCopyBackward0>)\n",
      "416: 176.11521911621094\n",
      "tensor(116.3579, grad_fn=<ToCopyBackward0>)\n",
      "417: 116.35786437988281\n",
      "tensor(163.5294, grad_fn=<ToCopyBackward0>)\n",
      "418: 163.52943420410156\n",
      "tensor(73.4792, grad_fn=<ToCopyBackward0>)\n",
      "419: 73.47918701171875\n",
      "tensor(103.7721, grad_fn=<ToCopyBackward0>)\n",
      "420: 103.77207946777344\n",
      "tensor(161.5294, grad_fn=<ToCopyBackward0>)\n",
      "421: 161.52943420410156\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "422: 56.60050582885742\n",
      "tensor(60.8934, grad_fn=<ToCopyBackward0>)\n",
      "423: 60.89339828491211\n",
      "tensor(44.3076, grad_fn=<ToCopyBackward0>)\n",
      "424: 44.307613372802734\n",
      "tensor(159.2365, grad_fn=<ToCopyBackward0>)\n",
      "425: 159.23654174804688\n",
      "tensor(61.7218, grad_fn=<ToCopyBackward0>)\n",
      "426: 61.721824645996094\n",
      "tensor(204.9939, grad_fn=<ToCopyBackward0>)\n",
      "427: 204.993896484375\n",
      "tensor(134.0650, grad_fn=<ToCopyBackward0>)\n",
      "428: 134.06497192382812\n",
      "tensor(89.7721, grad_fn=<ToCopyBackward0>)\n",
      "429: 89.77207946777344\n",
      "tensor(117.4792, grad_fn=<ToCopyBackward0>)\n",
      "430: 117.47918701171875\n",
      "tensor(10.8431, grad_fn=<ToCopyBackward0>)\n",
      "431: 10.843145370483398\n",
      "tensor(14.5503, grad_fn=<ToCopyBackward0>)\n",
      "432: 14.550252914428711\n",
      "tensor(68.8934, grad_fn=<ToCopyBackward0>)\n",
      "433: 68.89339447021484\n",
      "tensor(49.7218, grad_fn=<ToCopyBackward0>)\n",
      "434: 49.721824645996094\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "435: 112.35786437988281\n",
      "tensor(145.5294, grad_fn=<ToCopyBackward0>)\n",
      "436: 145.52943420410156\n",
      "tensor(20.5130, grad_fn=<ToCopyBackward0>)\n",
      "437: 20.51300048828125\n",
      "tensor(119.1863, grad_fn=<ToCopyBackward0>)\n",
      "438: 119.18629455566406\n",
      "tensor(109.7721, grad_fn=<ToCopyBackward0>)\n",
      "439: 109.77207946777344\n",
      "tensor(78.0147, grad_fn=<ToCopyBackward0>)\n",
      "440: 78.01471710205078\n",
      "tensor(97.7721, grad_fn=<ToCopyBackward0>)\n",
      "441: 97.77207946777344\n",
      "tensor(93.7721, grad_fn=<ToCopyBackward0>)\n",
      "442: 93.77207946777344\n",
      "tensor(104.0650, grad_fn=<ToCopyBackward0>)\n",
      "443: 104.06497192382812\n",
      "tensor(153.5294, grad_fn=<ToCopyBackward0>)\n",
      "444: 153.52943420410156\n",
      "tensor(12.5503, grad_fn=<ToCopyBackward0>)\n",
      "445: 12.550252914428711\n",
      "tensor(75.1863, grad_fn=<ToCopyBackward0>)\n",
      "446: 75.18629455566406\n",
      "tensor(148.9436, grad_fn=<ToCopyBackward0>)\n",
      "447: 148.9436492919922\n",
      "tensor(107.1863, grad_fn=<ToCopyBackward0>)\n",
      "448: 107.18629455566406\n",
      "tensor(47.4289, grad_fn=<ToCopyBackward0>)\n",
      "449: 47.428932189941406\n",
      "tensor(41.7218, grad_fn=<ToCopyBackward0>)\n",
      "450: 41.721824645996094\n",
      "tensor(88.6005, grad_fn=<ToCopyBackward0>)\n",
      "451: 88.60050201416016\n",
      "tensor(213.2868, grad_fn=<ToCopyBackward0>)\n",
      "452: 213.28680419921875\n",
      "tensor(121.7721, grad_fn=<ToCopyBackward0>)\n",
      "453: 121.77207946777344\n",
      "tensor(82.0147, grad_fn=<ToCopyBackward0>)\n",
      "454: 82.01471710205078\n",
      "tensor(108.0650, grad_fn=<ToCopyBackward0>)\n",
      "455: 108.06497192382812\n",
      "tensor(72.3076, grad_fn=<ToCopyBackward0>)\n",
      "456: 72.30760955810547\n",
      "tensor(14.8431, grad_fn=<ToCopyBackward0>)\n",
      "457: 14.843145370483398\n",
      "tensor(119.7532, grad_fn=<ToCopyBackward0>)\n",
      "458: 119.75321960449219\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "459: 112.35786437988281\n",
      "tensor(75.1863, grad_fn=<ToCopyBackward0>)\n",
      "460: 75.18629455566406\n",
      "tensor(147.8223, grad_fn=<ToCopyBackward0>)\n",
      "461: 147.82232666015625\n",
      "tensor(51.7218, grad_fn=<ToCopyBackward0>)\n",
      "462: 51.721824645996094\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "463: 128.6507568359375\n",
      "tensor(106.8934, grad_fn=<ToCopyBackward0>)\n",
      "464: 106.89339447021484\n",
      "tensor(16.5503, grad_fn=<ToCopyBackward0>)\n",
      "465: 16.55025291442871\n",
      "tensor(119.7721, grad_fn=<ToCopyBackward0>)\n",
      "466: 119.77207946777344\n",
      "tensor(182.7010, grad_fn=<ToCopyBackward0>)\n",
      "467: 182.7010040283203\n",
      "tensor(128.3579, grad_fn=<ToCopyBackward0>)\n",
      "468: 128.3578643798828\n",
      "tensor(165.8223, grad_fn=<ToCopyBackward0>)\n",
      "469: 165.82232666015625\n",
      "tensor(124.0650, grad_fn=<ToCopyBackward0>)\n",
      "470: 124.06497192382812\n",
      "tensor(133.2365, grad_fn=<ToCopyBackward0>)\n",
      "471: 133.23654174804688\n",
      "tensor(161.8223, grad_fn=<ToCopyBackward0>)\n",
      "472: 161.82232666015625\n",
      "tensor(166.4081, grad_fn=<ToCopyBackward0>)\n",
      "473: 166.40811157226562\n",
      "tensor(153.8223, grad_fn=<ToCopyBackward0>)\n",
      "474: 153.82232666015625\n",
      "tensor(103.1863, grad_fn=<ToCopyBackward0>)\n",
      "475: 103.18629455566406\n",
      "tensor(178.4081, grad_fn=<ToCopyBackward0>)\n",
      "476: 178.40811157226562\n",
      "tensor(223.5797, grad_fn=<ToCopyBackward0>)\n",
      "477: 223.57969665527344\n",
      "tensor(143.8223, grad_fn=<ToCopyBackward0>)\n",
      "478: 143.82232666015625\n",
      "tensor(25.7218, grad_fn=<ToCopyBackward0>)\n",
      "479: 25.721824645996094\n",
      "tensor(71.1697, grad_fn=<ToCopyBackward0>)\n",
      "480: 71.16970825195312\n",
      "tensor(83.1863, grad_fn=<ToCopyBackward0>)\n",
      "481: 83.18629455566406\n",
      "tensor(24.8431, grad_fn=<ToCopyBackward0>)\n",
      "482: 24.8431453704834\n",
      "tensor(113.7721, grad_fn=<ToCopyBackward0>)\n",
      "483: 113.77207946777344\n",
      "tensor(65.4792, grad_fn=<ToCopyBackward0>)\n",
      "484: 65.47918701171875\n",
      "tensor(16.5503, grad_fn=<ToCopyBackward0>)\n",
      "485: 16.55025291442871\n",
      "tensor(12.8431, grad_fn=<ToCopyBackward0>)\n",
      "486: 12.843145370483398\n",
      "tensor(134.0650, grad_fn=<ToCopyBackward0>)\n",
      "487: 134.06497192382812\n",
      "tensor(119.4792, grad_fn=<ToCopyBackward0>)\n",
      "488: 119.47918701171875\n",
      "tensor(64.6005, grad_fn=<ToCopyBackward0>)\n",
      "489: 64.60050201416016\n",
      "tensor(177.5294, grad_fn=<ToCopyBackward0>)\n",
      "490: 177.52943420410156\n",
      "tensor(46.3076, grad_fn=<ToCopyBackward0>)\n",
      "491: 46.307613372802734\n",
      "tensor(178.7010, grad_fn=<ToCopyBackward0>)\n",
      "492: 178.7010040283203\n",
      "tensor(193.8223, grad_fn=<ToCopyBackward0>)\n",
      "493: 193.82232666015625\n",
      "tensor(190.7010, grad_fn=<ToCopyBackward0>)\n",
      "494: 190.7010040283203\n",
      "tensor(107.1863, grad_fn=<ToCopyBackward0>)\n",
      "495: 107.18629455566406\n",
      "tensor(170.1152, grad_fn=<ToCopyBackward0>)\n",
      "496: 170.11521911621094\n",
      "tensor(27.7218, grad_fn=<ToCopyBackward0>)\n",
      "497: 27.721824645996094\n",
      "tensor(54.3076, grad_fn=<ToCopyBackward0>)\n",
      "498: 54.307613372802734\n",
      "tensor(157.5294, grad_fn=<ToCopyBackward0>)\n",
      "499: 157.52943420410156\n",
      "tensor(43.4289, grad_fn=<ToCopyBackward0>)\n",
      "500: 43.428932189941406\n",
      "tensor(58.3076, grad_fn=<ToCopyBackward0>)\n",
      "501: 58.307613372802734\n",
      "tensor(80.8934, grad_fn=<ToCopyBackward0>)\n",
      "502: 80.89340209960938\n",
      "tensor(95.4792, grad_fn=<ToCopyBackward0>)\n",
      "503: 95.47918701171875\n",
      "tensor(97.7721, grad_fn=<ToCopyBackward0>)\n",
      "504: 97.77207946777344\n",
      "tensor(178.1152, grad_fn=<ToCopyBackward0>)\n",
      "505: 178.11521911621094\n",
      "tensor(164.1152, grad_fn=<ToCopyBackward0>)\n",
      "506: 164.11521911621094\n",
      "tensor(105.7721, grad_fn=<ToCopyBackward0>)\n",
      "507: 105.77207946777344\n",
      "tensor(137.2365, grad_fn=<ToCopyBackward0>)\n",
      "508: 137.23654174804688\n",
      "tensor(175.5294, grad_fn=<ToCopyBackward0>)\n",
      "509: 175.52943420410156\n",
      "tensor(65.4792, grad_fn=<ToCopyBackward0>)\n",
      "510: 65.47918701171875\n",
      "tensor(104.6508, grad_fn=<ToCopyBackward0>)\n",
      "511: 104.6507568359375\n",
      "tensor(82.0650, grad_fn=<ToCopyBackward0>)\n",
      "512: 82.06497192382812\n",
      "tensor(94.3579, grad_fn=<ToCopyBackward0>)\n",
      "513: 94.35786437988281\n",
      "tensor(215.2868, grad_fn=<ToCopyBackward0>)\n",
      "514: 215.28680419921875\n",
      "tensor(206.9939, grad_fn=<ToCopyBackward0>)\n",
      "515: 206.993896484375\n",
      "tensor(14.8431, grad_fn=<ToCopyBackward0>)\n",
      "516: 14.843146324157715\n",
      "tensor(3.9645, grad_fn=<ToCopyBackward0>)\n",
      "517: 3.964465856552124\n",
      "tensor(157.8223, grad_fn=<ToCopyBackward0>)\n",
      "518: 157.82232666015625\n",
      "tensor(122.0650, grad_fn=<ToCopyBackward0>)\n",
      "519: 122.06497192382812\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "520: 56.60050582885742\n",
      "tensor(105.4792, grad_fn=<ToCopyBackward0>)\n",
      "521: 105.47918701171875\n",
      "tensor(122.0650, grad_fn=<ToCopyBackward0>)\n",
      "522: 122.06497192382812\n",
      "tensor(60.8934, grad_fn=<ToCopyBackward0>)\n",
      "523: 60.89339828491211\n",
      "tensor(114.9436, grad_fn=<ToCopyBackward0>)\n",
      "524: 114.94364929199219\n",
      "tensor(64.0147, grad_fn=<ToCopyBackward0>)\n",
      "525: 64.01471710205078\n",
      "tensor(98.3579, grad_fn=<ToCopyBackward0>)\n",
      "526: 98.35786437988281\n",
      "tensor(221.2868, grad_fn=<ToCopyBackward0>)\n",
      "527: 221.2867889404297\n",
      "tensor(134.9436, grad_fn=<ToCopyBackward0>)\n",
      "528: 134.9436492919922\n",
      "tensor(134.0650, grad_fn=<ToCopyBackward0>)\n",
      "529: 134.06497192382812\n",
      "tensor(110.0650, grad_fn=<ToCopyBackward0>)\n",
      "530: 110.06497192382812\n",
      "tensor(23.1360, grad_fn=<ToCopyBackward0>)\n",
      "531: 23.136037826538086\n",
      "tensor(180.1152, grad_fn=<ToCopyBackward0>)\n",
      "532: 180.11521911621094\n",
      "tensor(12.8431, grad_fn=<ToCopyBackward0>)\n",
      "533: 12.843145370483398\n",
      "tensor(66.8934, grad_fn=<ToCopyBackward0>)\n",
      "534: 66.89340209960938\n",
      "tensor(124.6508, grad_fn=<ToCopyBackward0>)\n",
      "535: 124.6507568359375\n",
      "tensor(149.2365, grad_fn=<ToCopyBackward0>)\n",
      "536: 149.23654174804688\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "537: 71.18629455566406\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "538: 91.18629455566406\n",
      "tensor(207.2868, grad_fn=<ToCopyBackward0>)\n",
      "539: 207.2867889404297\n",
      "tensor(113.7721, grad_fn=<ToCopyBackward0>)\n",
      "540: 113.77207946777344\n",
      "tensor(56.3076, grad_fn=<ToCopyBackward0>)\n",
      "541: 56.307613372802734\n",
      "tensor(82.3076, grad_fn=<ToCopyBackward0>)\n",
      "542: 82.30760955810547\n",
      "tensor(151.2365, grad_fn=<ToCopyBackward0>)\n",
      "543: 151.23654174804688\n",
      "tensor(64.8934, grad_fn=<ToCopyBackward0>)\n",
      "544: 64.89340209960938\n",
      "tensor(161.8223, grad_fn=<ToCopyBackward0>)\n",
      "545: 161.82232666015625\n",
      "tensor(142.9436, grad_fn=<ToCopyBackward0>)\n",
      "546: 142.9436492919922\n",
      "tensor(32.8431, grad_fn=<ToCopyBackward0>)\n",
      "547: 32.84314727783203\n",
      "tensor(136.0650, grad_fn=<ToCopyBackward0>)\n",
      "548: 136.06497192382812\n",
      "tensor(137.2365, grad_fn=<ToCopyBackward0>)\n",
      "549: 137.23654174804688\n",
      "tensor(60.6005, grad_fn=<ToCopyBackward0>)\n",
      "550: 60.60050582885742\n",
      "tensor(208.1152, grad_fn=<ToCopyBackward0>)\n",
      "551: 208.11521911621094\n",
      "tensor(108.9436, grad_fn=<ToCopyBackward0>)\n",
      "552: 108.94364929199219\n",
      "tensor(129.2365, grad_fn=<ToCopyBackward0>)\n",
      "553: 129.23654174804688\n",
      "tensor(137.5294, grad_fn=<ToCopyBackward0>)\n",
      "554: 137.52943420410156\n",
      "tensor(150.9436, grad_fn=<ToCopyBackward0>)\n",
      "555: 150.9436492919922\n",
      "tensor(66.6005, grad_fn=<ToCopyBackward0>)\n",
      "556: 66.60050201416016\n",
      "tensor(84.0650, grad_fn=<ToCopyBackward0>)\n",
      "557: 84.06497192382812\n",
      "tensor(48.6005, grad_fn=<ToCopyBackward0>)\n",
      "558: 48.60050582885742\n",
      "tensor(68.6005, grad_fn=<ToCopyBackward0>)\n",
      "559: 68.60050201416016\n",
      "tensor(83.4792, grad_fn=<ToCopyBackward0>)\n",
      "560: 83.47918701171875\n",
      "tensor(142.9436, grad_fn=<ToCopyBackward0>)\n",
      "561: 142.9436492919922\n",
      "tensor(124.6508, grad_fn=<ToCopyBackward0>)\n",
      "562: 124.6507568359375\n",
      "tensor(186.7010, grad_fn=<ToCopyBackward0>)\n",
      "563: 186.7010040283203\n",
      "tensor(120.0650, grad_fn=<ToCopyBackward0>)\n",
      "564: 120.06497192382812\n",
      "tensor(78.8934, grad_fn=<ToCopyBackward0>)\n",
      "565: 78.89339447021484\n",
      "tensor(48.0147, grad_fn=<ToCopyBackward0>)\n",
      "566: 48.01471710205078\n",
      "tensor(148.9436, grad_fn=<ToCopyBackward0>)\n",
      "567: 148.9436492919922\n",
      "tensor(114.6508, grad_fn=<ToCopyBackward0>)\n",
      "568: 114.6507568359375\n",
      "tensor(5.6716, grad_fn=<ToCopyBackward0>)\n",
      "569: 5.671572685241699\n",
      "tensor(120.0650, grad_fn=<ToCopyBackward0>)\n",
      "570: 120.06497192382812\n",
      "tensor(79.4792, grad_fn=<ToCopyBackward0>)\n",
      "571: 79.47918701171875\n",
      "tensor(106.0650, grad_fn=<ToCopyBackward0>)\n",
      "572: 106.06497192382812\n",
      "tensor(27.1360, grad_fn=<ToCopyBackward0>)\n",
      "573: 27.136037826538086\n",
      "tensor(107.1863, grad_fn=<ToCopyBackward0>)\n",
      "574: 107.18629455566406\n",
      "tensor(124.6508, grad_fn=<ToCopyBackward0>)\n",
      "575: 124.6507568359375\n",
      "tensor(52.6005, grad_fn=<ToCopyBackward0>)\n",
      "576: 52.60050582885742\n",
      "tensor(135.2365, grad_fn=<ToCopyBackward0>)\n",
      "577: 135.23654174804688\n",
      "tensor(42.0147, grad_fn=<ToCopyBackward0>)\n",
      "578: 42.01471710205078\n",
      "tensor(147.8223, grad_fn=<ToCopyBackward0>)\n",
      "579: 147.82232666015625\n",
      "tensor(132.6508, grad_fn=<ToCopyBackward0>)\n",
      "580: 132.6507568359375\n",
      "tensor(66.3076, grad_fn=<ToCopyBackward0>)\n",
      "581: 66.30760955810547\n",
      "tensor(140.6508, grad_fn=<ToCopyBackward0>)\n",
      "582: 140.6507568359375\n",
      "tensor(76.5862, grad_fn=<ToCopyBackward0>)\n",
      "583: 76.58619689941406\n",
      "tensor(178.4081, grad_fn=<ToCopyBackward0>)\n",
      "584: 178.40811157226562\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "585: 102.06497192382812\n",
      "tensor(27.4289, grad_fn=<ToCopyBackward0>)\n",
      "586: 27.428932189941406\n",
      "tensor(13.6716, grad_fn=<ToCopyBackward0>)\n",
      "587: 13.6715726852417\n",
      "tensor(221.5797, grad_fn=<ToCopyBackward0>)\n",
      "588: 221.57969665527344\n",
      "tensor(89.7721, grad_fn=<ToCopyBackward0>)\n",
      "589: 89.77207946777344\n",
      "tensor(124.3579, grad_fn=<ToCopyBackward0>)\n",
      "590: 124.35786437988281\n",
      "tensor(191.7955, grad_fn=<ToCopyBackward0>)\n",
      "591: 191.79551696777344\n",
      "tensor(118.3579, grad_fn=<ToCopyBackward0>)\n",
      "592: 118.35786437988281\n",
      "tensor(73.4792, grad_fn=<ToCopyBackward0>)\n",
      "593: 73.47918701171875\n",
      "tensor(161.8223, grad_fn=<ToCopyBackward0>)\n",
      "594: 161.82232666015625\n",
      "tensor(31.4289, grad_fn=<ToCopyBackward0>)\n",
      "595: 31.428932189941406\n",
      "tensor(132.6508, grad_fn=<ToCopyBackward0>)\n",
      "596: 132.6507568359375\n",
      "tensor(105.7721, grad_fn=<ToCopyBackward0>)\n",
      "597: 105.77207946777344\n",
      "tensor(188.7010, grad_fn=<ToCopyBackward0>)\n",
      "598: 188.7010040283203\n",
      "tensor(137.5294, grad_fn=<ToCopyBackward0>)\n",
      "599: 137.52943420410156\n",
      "tensor(66.6005, grad_fn=<ToCopyBackward0>)\n",
      "600: 66.60050201416016\n",
      "tensor(75.7218, grad_fn=<ToCopyBackward0>)\n",
      "601: 75.7218246459961\n",
      "tensor(79.7721, grad_fn=<ToCopyBackward0>)\n",
      "602: 79.77207946777344\n",
      "tensor(44.8431, grad_fn=<ToCopyBackward0>)\n",
      "603: 44.84314727783203\n",
      "tensor(53.8737, grad_fn=<ToCopyBackward0>)\n",
      "604: 53.873680114746094\n",
      "tensor(162.1152, grad_fn=<ToCopyBackward0>)\n",
      "605: 162.11521911621094\n",
      "tensor(133.2365, grad_fn=<ToCopyBackward0>)\n",
      "606: 133.23654174804688\n",
      "tensor(39.4289, grad_fn=<ToCopyBackward0>)\n",
      "607: 39.428932189941406\n",
      "tensor(83.4792, grad_fn=<ToCopyBackward0>)\n",
      "608: 83.47918701171875\n",
      "tensor(97.1863, grad_fn=<ToCopyBackward0>)\n",
      "609: 97.18629455566406\n",
      "tensor(123.2365, grad_fn=<ToCopyBackward0>)\n",
      "610: 123.2365493774414\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "611: 102.06497955322266\n",
      "tensor(161.2366, grad_fn=<ToCopyBackward0>)\n",
      "612: 161.23655700683594\n",
      "tensor(72.0147, grad_fn=<ToCopyBackward0>)\n",
      "613: 72.01472473144531\n",
      "tensor(138.6508, grad_fn=<ToCopyBackward0>)\n",
      "614: 138.65077209472656\n",
      "tensor(168.1152, grad_fn=<ToCopyBackward0>)\n",
      "615: 168.115234375\n",
      "tensor(90.8934, grad_fn=<ToCopyBackward0>)\n",
      "616: 90.89340209960938\n",
      "tensor(83.4792, grad_fn=<ToCopyBackward0>)\n",
      "617: 83.47918701171875\n",
      "tensor(153.5294, grad_fn=<ToCopyBackward0>)\n",
      "618: 153.52943420410156\n",
      "tensor(85.4792, grad_fn=<ToCopyBackward0>)\n",
      "619: 85.47918701171875\n",
      "tensor(61.7218, grad_fn=<ToCopyBackward0>)\n",
      "620: 61.721824645996094\n",
      "tensor(124.3579, grad_fn=<ToCopyBackward0>)\n",
      "621: 124.35786437988281\n",
      "tensor(79.1863, grad_fn=<ToCopyBackward0>)\n",
      "622: 79.18629455566406\n",
      "tensor(72.3076, grad_fn=<ToCopyBackward0>)\n",
      "623: 72.30760955810547\n",
      "tensor(194.4081, grad_fn=<ToCopyBackward0>)\n",
      "624: 194.40811157226562\n",
      "tensor(84.0650, grad_fn=<ToCopyBackward0>)\n",
      "625: 84.06497192382812\n",
      "tensor(72.8934, grad_fn=<ToCopyBackward0>)\n",
      "626: 72.89340209960938\n",
      "tensor(121.7721, grad_fn=<ToCopyBackward0>)\n",
      "627: 121.77207946777344\n",
      "tensor(60.6005, grad_fn=<ToCopyBackward0>)\n",
      "628: 60.60050582885742\n",
      "tensor(17.0764, grad_fn=<ToCopyBackward0>)\n",
      "629: 17.07638931274414\n",
      "tensor(103.7721, grad_fn=<ToCopyBackward0>)\n",
      "630: 103.77207946777344\n",
      "tensor(116.6508, grad_fn=<ToCopyBackward0>)\n",
      "631: 116.6507568359375\n",
      "tensor(47.7218, grad_fn=<ToCopyBackward0>)\n",
      "632: 47.721824645996094\n",
      "tensor(77.7218, grad_fn=<ToCopyBackward0>)\n",
      "633: 77.7218246459961\n",
      "tensor(108.3579, grad_fn=<ToCopyBackward0>)\n",
      "634: 108.35786437988281\n",
      "tensor(48.3076, grad_fn=<ToCopyBackward0>)\n",
      "635: 48.307613372802734\n",
      "tensor(176.4081, grad_fn=<ToCopyBackward0>)\n",
      "636: 176.40811157226562\n",
      "tensor(93.4792, grad_fn=<ToCopyBackward0>)\n",
      "637: 93.47918701171875\n",
      "tensor(90.3579, grad_fn=<ToCopyBackward0>)\n",
      "638: 90.35786437988281\n",
      "tensor(73.1863, grad_fn=<ToCopyBackward0>)\n",
      "639: 73.18629455566406\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "640: 56.60050582885742\n",
      "tensor(143.2365, grad_fn=<ToCopyBackward0>)\n",
      "641: 143.23654174804688\n",
      "tensor(200.7010, grad_fn=<ToCopyBackward0>)\n",
      "642: 200.7010040283203\n",
      "tensor(151.5294, grad_fn=<ToCopyBackward0>)\n",
      "643: 151.52943420410156\n",
      "tensor(35.4289, grad_fn=<ToCopyBackward0>)\n",
      "644: 35.428932189941406\n",
      "tensor(84.6005, grad_fn=<ToCopyBackward0>)\n",
      "645: 84.60050201416016\n",
      "tensor(178.4081, grad_fn=<ToCopyBackward0>)\n",
      "646: 178.40811157226562\n",
      "tensor(47.7218, grad_fn=<ToCopyBackward0>)\n",
      "647: 47.721824645996094\n",
      "tensor(132.9436, grad_fn=<ToCopyBackward0>)\n",
      "648: 132.9436492919922\n",
      "tensor(116.3579, grad_fn=<ToCopyBackward0>)\n",
      "649: 116.35786437988281\n",
      "tensor(165.2365, grad_fn=<ToCopyBackward0>)\n",
      "650: 165.23654174804688\n",
      "tensor(74.8934, grad_fn=<ToCopyBackward0>)\n",
      "651: 74.89339447021484\n",
      "tensor(124.0650, grad_fn=<ToCopyBackward0>)\n",
      "652: 124.06497192382812\n",
      "tensor(85.4792, grad_fn=<ToCopyBackward0>)\n",
      "653: 85.47918701171875\n",
      "tensor(145.5294, grad_fn=<ToCopyBackward0>)\n",
      "654: 145.52943420410156\n",
      "tensor(157.5294, grad_fn=<ToCopyBackward0>)\n",
      "655: 157.52943420410156\n",
      "tensor(138.9436, grad_fn=<ToCopyBackward0>)\n",
      "656: 138.9436492919922\n",
      "tensor(146.9436, grad_fn=<ToCopyBackward0>)\n",
      "657: 146.9436492919922\n",
      "tensor(35.1360, grad_fn=<ToCopyBackward0>)\n",
      "658: 35.13603973388672\n",
      "tensor(108.0650, grad_fn=<ToCopyBackward0>)\n",
      "659: 108.06497192382812\n",
      "tensor(178.1152, grad_fn=<ToCopyBackward0>)\n",
      "660: 178.11521911621094\n",
      "tensor(181.5294, grad_fn=<ToCopyBackward0>)\n",
      "661: 181.52943420410156\n",
      "tensor(101.7721, grad_fn=<ToCopyBackward0>)\n",
      "662: 101.77207946777344\n",
      "tensor(70.8934, grad_fn=<ToCopyBackward0>)\n",
      "663: 70.89339447021484\n",
      "tensor(165.2365, grad_fn=<ToCopyBackward0>)\n",
      "664: 165.23654174804688\n",
      "tensor(46.0147, grad_fn=<ToCopyBackward0>)\n",
      "665: 46.01471710205078\n",
      "tensor(126.9436, grad_fn=<ToCopyBackward0>)\n",
      "666: 126.94364929199219\n",
      "tensor(184.4081, grad_fn=<ToCopyBackward0>)\n",
      "667: 184.40811157226562\n",
      "tensor(157.5294, grad_fn=<ToCopyBackward0>)\n",
      "668: 157.52943420410156\n",
      "tensor(134.0650, grad_fn=<ToCopyBackward0>)\n",
      "669: 134.06497192382812\n",
      "tensor(29.7218, grad_fn=<ToCopyBackward0>)\n",
      "670: 29.721824645996094\n",
      "tensor(77.7721, grad_fn=<ToCopyBackward0>)\n",
      "671: 77.77207946777344\n",
      "tensor(69.4792, grad_fn=<ToCopyBackward0>)\n",
      "672: 69.47918701171875\n",
      "tensor(25.7218, grad_fn=<ToCopyBackward0>)\n",
      "673: 25.721824645996094\n",
      "tensor(22.2574, grad_fn=<ToCopyBackward0>)\n",
      "674: 22.25735855102539\n",
      "tensor(121.7721, grad_fn=<ToCopyBackward0>)\n",
      "675: 121.77207946777344\n",
      "tensor(77.7721, grad_fn=<ToCopyBackward0>)\n",
      "676: 77.77207946777344\n",
      "tensor(79.7721, grad_fn=<ToCopyBackward0>)\n",
      "677: 79.77207946777344\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "678: 91.18629455566406\n",
      "tensor(157.5294, grad_fn=<ToCopyBackward0>)\n",
      "679: 157.52943420410156\n",
      "tensor(108.9436, grad_fn=<ToCopyBackward0>)\n",
      "680: 108.94364929199219\n",
      "tensor(46.0147, grad_fn=<ToCopyBackward0>)\n",
      "681: 46.01471710205078\n",
      "tensor(122.9436, grad_fn=<ToCopyBackward0>)\n",
      "682: 122.94364929199219\n",
      "tensor(163.5294, grad_fn=<ToCopyBackward0>)\n",
      "683: 163.52943420410156\n",
      "tensor(84.6005, grad_fn=<ToCopyBackward0>)\n",
      "684: 84.60050201416016\n",
      "tensor(116.9436, grad_fn=<ToCopyBackward0>)\n",
      "685: 116.94364929199219\n",
      "tensor(167.8223, grad_fn=<ToCopyBackward0>)\n",
      "686: 167.82232666015625\n",
      "tensor(56.3076, grad_fn=<ToCopyBackward0>)\n",
      "687: 56.307613372802734\n",
      "tensor(48.6005, grad_fn=<ToCopyBackward0>)\n",
      "688: 48.60050582885742\n",
      "tensor(96.8934, grad_fn=<ToCopyBackward0>)\n",
      "689: 96.89340209960938\n",
      "tensor(37.7218, grad_fn=<ToCopyBackward0>)\n",
      "690: 37.721824645996094\n",
      "tensor(133.2365, grad_fn=<ToCopyBackward0>)\n",
      "691: 133.23654174804688\n",
      "tensor(58.8431, grad_fn=<ToCopyBackward0>)\n",
      "692: 58.84314727783203\n",
      "tensor(139.2365, grad_fn=<ToCopyBackward0>)\n",
      "693: 139.23654174804688\n",
      "tensor(120.3579, grad_fn=<ToCopyBackward0>)\n",
      "694: 120.35786437988281\n",
      "tensor(54.0147, grad_fn=<ToCopyBackward0>)\n",
      "695: 54.01471710205078\n",
      "tensor(78.0650, grad_fn=<ToCopyBackward0>)\n",
      "696: 78.06497192382812\n",
      "tensor(126.3579, grad_fn=<ToCopyBackward0>)\n",
      "697: 126.35786437988281\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "698: 56.60050582885742\n",
      "tensor(155.2365, grad_fn=<ToCopyBackward0>)\n",
      "699: 155.23654174804688\n",
      "tensor(72.3076, grad_fn=<ToCopyBackward0>)\n",
      "700: 72.30760955810547\n",
      "tensor(117.1863, grad_fn=<ToCopyBackward0>)\n",
      "701: 117.18629455566406\n",
      "tensor(107.4792, grad_fn=<ToCopyBackward0>)\n",
      "702: 107.47918701171875\n",
      "tensor(35.7218, grad_fn=<ToCopyBackward0>)\n",
      "703: 35.721824645996094\n",
      "tensor(97.7721, grad_fn=<ToCopyBackward0>)\n",
      "704: 97.77207946777344\n",
      "tensor(141.2365, grad_fn=<ToCopyBackward0>)\n",
      "705: 141.23654174804688\n",
      "tensor(117.4792, grad_fn=<ToCopyBackward0>)\n",
      "706: 117.47918701171875\n",
      "tensor(118.9436, grad_fn=<ToCopyBackward0>)\n",
      "707: 118.94364929199219\n",
      "tensor(93.4792, grad_fn=<ToCopyBackward0>)\n",
      "708: 93.47918701171875\n",
      "tensor(124.6508, grad_fn=<ToCopyBackward0>)\n",
      "709: 124.6507568359375\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "710: 102.06497192382812\n",
      "tensor(66.3076, grad_fn=<ToCopyBackward0>)\n",
      "711: 66.30760955810547\n",
      "tensor(149.2365, grad_fn=<ToCopyBackward0>)\n",
      "712: 149.23654174804688\n",
      "tensor(105.7721, grad_fn=<ToCopyBackward0>)\n",
      "713: 105.77207946777344\n",
      "tensor(58.3076, grad_fn=<ToCopyBackward0>)\n",
      "714: 58.307613372802734\n",
      "tensor(114.9436, grad_fn=<ToCopyBackward0>)\n",
      "715: 114.94364929199219\n",
      "tensor(184.4081, grad_fn=<ToCopyBackward0>)\n",
      "716: 184.40811157226562\n",
      "tensor(72.0027, grad_fn=<ToCopyBackward0>)\n",
      "717: 72.00267791748047\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "718: 128.6507568359375\n",
      "tensor(64.8934, grad_fn=<ToCopyBackward0>)\n",
      "719: 64.89339447021484\n",
      "tensor(58.6005, grad_fn=<ToCopyBackward0>)\n",
      "720: 58.60050582885742\n",
      "tensor(214.9939, grad_fn=<ToCopyBackward0>)\n",
      "721: 214.993896484375\n",
      "tensor(28.8431, grad_fn=<ToCopyBackward0>)\n",
      "722: 28.8431453704834\n",
      "tensor(40.3076, grad_fn=<ToCopyBackward0>)\n",
      "723: 40.307613372802734\n",
      "tensor(48.2944, grad_fn=<ToCopyBackward0>)\n",
      "724: 48.29443359375\n",
      "tensor(120.0650, grad_fn=<ToCopyBackward0>)\n",
      "725: 120.06497192382812\n",
      "tensor(214.9939, grad_fn=<ToCopyBackward0>)\n",
      "726: 214.993896484375\n",
      "tensor(102.6508, grad_fn=<ToCopyBackward0>)\n",
      "727: 102.6507568359375\n",
      "tensor(60.6005, grad_fn=<ToCopyBackward0>)\n",
      "728: 60.60050582885742\n",
      "tensor(58.8934, grad_fn=<ToCopyBackward0>)\n",
      "729: 58.89339828491211\n",
      "tensor(153.5294, grad_fn=<ToCopyBackward0>)\n",
      "730: 153.52943420410156\n",
      "tensor(95.1863, grad_fn=<ToCopyBackward0>)\n",
      "731: 95.18629455566406\n",
      "tensor(131.2365, grad_fn=<ToCopyBackward0>)\n",
      "732: 131.23654174804688\n",
      "tensor(110.0650, grad_fn=<ToCopyBackward0>)\n",
      "733: 110.06497192382812\n",
      "tensor(58.8934, grad_fn=<ToCopyBackward0>)\n",
      "734: 58.89339828491211\n",
      "tensor(146.9436, grad_fn=<ToCopyBackward0>)\n",
      "735: 146.9436492919922\n",
      "tensor(102.8934, grad_fn=<ToCopyBackward0>)\n",
      "736: 102.89339447021484\n",
      "tensor(68.6005, grad_fn=<ToCopyBackward0>)\n",
      "737: 68.60050201416016\n",
      "tensor(64.6005, grad_fn=<ToCopyBackward0>)\n",
      "738: 64.60050201416016\n",
      "tensor(69.4792, grad_fn=<ToCopyBackward0>)\n",
      "739: 69.47918701171875\n",
      "tensor(77.7721, grad_fn=<ToCopyBackward0>)\n",
      "740: 77.77207946777344\n",
      "tensor(105.4792, grad_fn=<ToCopyBackward0>)\n",
      "741: 105.47918701171875\n",
      "tensor(5.6716, grad_fn=<ToCopyBackward0>)\n",
      "742: 5.671572685241699\n",
      "tensor(101.7721, grad_fn=<ToCopyBackward0>)\n",
      "743: 101.77207946777344\n",
      "tensor(79.1863, grad_fn=<ToCopyBackward0>)\n",
      "744: 79.18629455566406\n",
      "tensor(81.4792, grad_fn=<ToCopyBackward0>)\n",
      "745: 81.47918701171875\n",
      "tensor(85.7721, grad_fn=<ToCopyBackward0>)\n",
      "746: 85.77207946777344\n",
      "tensor(99.4792, grad_fn=<ToCopyBackward0>)\n",
      "747: 99.47918701171875\n",
      "tensor(89.4792, grad_fn=<ToCopyBackward0>)\n",
      "748: 89.47918701171875\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "749: 128.6507568359375\n",
      "tensor(73.7721, grad_fn=<ToCopyBackward0>)\n",
      "750: 73.77207946777344\n",
      "tensor(159.2365, grad_fn=<ToCopyBackward0>)\n",
      "751: 159.23654174804688\n",
      "tensor(45.7218, grad_fn=<ToCopyBackward0>)\n",
      "752: 45.721824645996094\n",
      "tensor(69.4792, grad_fn=<ToCopyBackward0>)\n",
      "753: 69.47918701171875\n",
      "tensor(13.8466, grad_fn=<ToCopyBackward0>)\n",
      "754: 13.846582412719727\n",
      "tensor(96.3579, grad_fn=<ToCopyBackward0>)\n",
      "755: 96.35786437988281\n",
      "tensor(125.2365, grad_fn=<ToCopyBackward0>)\n",
      "756: 125.23654174804688\n",
      "tensor(27.4289, grad_fn=<ToCopyBackward0>)\n",
      "757: 27.42893409729004\n",
      "tensor(194.4081, grad_fn=<ToCopyBackward0>)\n",
      "758: 194.4081268310547\n",
      "tensor(103.1863, grad_fn=<ToCopyBackward0>)\n",
      "759: 103.18629455566406\n",
      "tensor(35.7218, grad_fn=<ToCopyBackward0>)\n",
      "760: 35.72182846069336\n",
      "tensor(133.7721, grad_fn=<ToCopyBackward0>)\n",
      "761: 133.77207946777344\n",
      "tensor(116.3579, grad_fn=<ToCopyBackward0>)\n",
      "762: 116.35787200927734\n",
      "tensor(159.2366, grad_fn=<ToCopyBackward0>)\n",
      "763: 159.23655700683594\n",
      "tensor(44.8431, grad_fn=<ToCopyBackward0>)\n",
      "764: 44.84314727783203\n",
      "tensor(108.3579, grad_fn=<ToCopyBackward0>)\n",
      "765: 108.35786437988281\n",
      "tensor(198.9939, grad_fn=<ToCopyBackward0>)\n",
      "766: 198.99391174316406\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "767: 102.06497192382812\n",
      "tensor(81.4792, grad_fn=<ToCopyBackward0>)\n",
      "768: 81.47918701171875\n",
      "tensor(160.1152, grad_fn=<ToCopyBackward0>)\n",
      "769: 160.11521911621094\n",
      "tensor(164.4081, grad_fn=<ToCopyBackward0>)\n",
      "770: 164.40811157226562\n",
      "tensor(140.6508, grad_fn=<ToCopyBackward0>)\n",
      "771: 140.6507568359375\n",
      "tensor(48.0147, grad_fn=<ToCopyBackward0>)\n",
      "772: 48.01471710205078\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "773: 56.60050582885742\n",
      "tensor(45.7218, grad_fn=<ToCopyBackward0>)\n",
      "774: 45.721824645996094\n",
      "tensor(110.9436, grad_fn=<ToCopyBackward0>)\n",
      "775: 110.94364929199219\n",
      "tensor(46.3076, grad_fn=<ToCopyBackward0>)\n",
      "776: 46.307613372802734\n",
      "tensor(134.9436, grad_fn=<ToCopyBackward0>)\n",
      "777: 134.9436492919922\n",
      "tensor(100.0650, grad_fn=<ToCopyBackward0>)\n",
      "778: 100.06497192382812\n",
      "tensor(31.4289, grad_fn=<ToCopyBackward0>)\n",
      "779: 31.428932189941406\n",
      "tensor(141.5294, grad_fn=<ToCopyBackward0>)\n",
      "780: 141.52943420410156\n",
      "tensor(133.7721, grad_fn=<ToCopyBackward0>)\n",
      "781: 133.77207946777344\n",
      "tensor(74.6005, grad_fn=<ToCopyBackward0>)\n",
      "782: 74.60050201416016\n",
      "tensor(3.6716, grad_fn=<ToCopyBackward0>)\n",
      "783: 3.6715729236602783\n",
      "tensor(100.3579, grad_fn=<ToCopyBackward0>)\n",
      "784: 100.35786437988281\n",
      "tensor(81.7721, grad_fn=<ToCopyBackward0>)\n",
      "785: 81.77207946777344\n",
      "tensor(126.3579, grad_fn=<ToCopyBackward0>)\n",
      "786: 126.35786437988281\n",
      "tensor(44.6005, grad_fn=<ToCopyBackward0>)\n",
      "787: 44.60050582885742\n",
      "tensor(206.9939, grad_fn=<ToCopyBackward0>)\n",
      "788: 206.993896484375\n",
      "tensor(116.0650, grad_fn=<ToCopyBackward0>)\n",
      "789: 116.06497192382812\n",
      "tensor(58.3076, grad_fn=<ToCopyBackward0>)\n",
      "790: 58.307613372802734\n",
      "tensor(194.9939, grad_fn=<ToCopyBackward0>)\n",
      "791: 194.993896484375\n",
      "tensor(132.9436, grad_fn=<ToCopyBackward0>)\n",
      "792: 132.9436492919922\n",
      "tensor(151.8223, grad_fn=<ToCopyBackward0>)\n",
      "793: 151.82232666015625\n",
      "tensor(52.3076, grad_fn=<ToCopyBackward0>)\n",
      "794: 52.30760955810547\n",
      "tensor(149.8223, grad_fn=<ToCopyBackward0>)\n",
      "795: 149.82232666015625\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "796: 112.35786437988281\n",
      "tensor(192.9939, grad_fn=<ToCopyBackward0>)\n",
      "797: 192.993896484375\n",
      "tensor(140.3579, grad_fn=<ToCopyBackward0>)\n",
      "798: 140.3578643798828\n",
      "tensor(85.7721, grad_fn=<ToCopyBackward0>)\n",
      "799: 85.77207946777344\n",
      "tensor(31.7218, grad_fn=<ToCopyBackward0>)\n",
      "800: 31.721826553344727\n",
      "tensor(78.8934, grad_fn=<ToCopyBackward0>)\n",
      "801: 78.89340209960938\n",
      "tensor(27.4289, grad_fn=<ToCopyBackward0>)\n",
      "802: 27.428932189941406\n",
      "tensor(209.2868, grad_fn=<ToCopyBackward0>)\n",
      "803: 209.28680419921875\n",
      "tensor(100.3579, grad_fn=<ToCopyBackward0>)\n",
      "804: 100.35786437988281\n",
      "tensor(35.4289, grad_fn=<ToCopyBackward0>)\n",
      "805: 35.428932189941406\n",
      "tensor(110.3579, grad_fn=<ToCopyBackward0>)\n",
      "806: 110.35786437988281\n",
      "tensor(76.6005, grad_fn=<ToCopyBackward0>)\n",
      "807: 76.60050201416016\n",
      "tensor(44.8431, grad_fn=<ToCopyBackward0>)\n",
      "808: 44.84314727783203\n",
      "tensor(21.1360, grad_fn=<ToCopyBackward0>)\n",
      "809: 21.136037826538086\n",
      "tensor(112.6508, grad_fn=<ToCopyBackward0>)\n",
      "810: 112.6507568359375\n",
      "tensor(110.0650, grad_fn=<ToCopyBackward0>)\n",
      "811: 110.06497192382812\n",
      "tensor(109.7532, grad_fn=<ToCopyBackward0>)\n",
      "812: 109.75321197509766\n",
      "tensor(66.0147, grad_fn=<ToCopyBackward0>)\n",
      "813: 66.01471710205078\n",
      "tensor(69.4792, grad_fn=<ToCopyBackward0>)\n",
      "814: 69.47918701171875\n",
      "tensor(23.4289, grad_fn=<ToCopyBackward0>)\n",
      "815: 23.428932189941406\n",
      "tensor(157.8223, grad_fn=<ToCopyBackward0>)\n",
      "816: 157.82232666015625\n",
      "tensor(116.6508, grad_fn=<ToCopyBackward0>)\n",
      "817: 116.6507568359375\n",
      "tensor(98.0650, grad_fn=<ToCopyBackward0>)\n",
      "818: 98.06497192382812\n",
      "tensor(79.1863, grad_fn=<ToCopyBackward0>)\n",
      "819: 79.18629455566406\n",
      "tensor(221.5797, grad_fn=<ToCopyBackward0>)\n",
      "820: 221.57969665527344\n",
      "tensor(163.5294, grad_fn=<ToCopyBackward0>)\n",
      "821: 163.52943420410156\n",
      "tensor(29.1360, grad_fn=<ToCopyBackward0>)\n",
      "822: 29.136037826538086\n",
      "tensor(198.7010, grad_fn=<ToCopyBackward0>)\n",
      "823: 198.7010040283203\n",
      "tensor(196.4081, grad_fn=<ToCopyBackward0>)\n",
      "824: 196.40811157226562\n",
      "tensor(106.3579, grad_fn=<ToCopyBackward0>)\n",
      "825: 106.35786437988281\n",
      "tensor(184.1152, grad_fn=<ToCopyBackward0>)\n",
      "826: 184.11521911621094\n",
      "tensor(39.4289, grad_fn=<ToCopyBackward0>)\n",
      "827: 39.428932189941406\n",
      "tensor(60.6005, grad_fn=<ToCopyBackward0>)\n",
      "828: 60.60050582885742\n",
      "tensor(3.9645, grad_fn=<ToCopyBackward0>)\n",
      "829: 3.964465856552124\n",
      "tensor(113.7721, grad_fn=<ToCopyBackward0>)\n",
      "830: 113.77207946777344\n",
      "tensor(87.7721, grad_fn=<ToCopyBackward0>)\n",
      "831: 87.77207946777344\n",
      "tensor(138.6508, grad_fn=<ToCopyBackward0>)\n",
      "832: 138.6507568359375\n",
      "tensor(127.2365, grad_fn=<ToCopyBackward0>)\n",
      "833: 127.23654174804688\n",
      "tensor(78.8934, grad_fn=<ToCopyBackward0>)\n",
      "834: 78.89339447021484\n",
      "tensor(124.9436, grad_fn=<ToCopyBackward0>)\n",
      "835: 124.94364929199219\n",
      "tensor(44.6005, grad_fn=<ToCopyBackward0>)\n",
      "836: 44.60050582885742\n",
      "tensor(54.8934, grad_fn=<ToCopyBackward0>)\n",
      "837: 54.89339828491211\n",
      "tensor(110.0650, grad_fn=<ToCopyBackward0>)\n",
      "838: 110.06497192382812\n",
      "tensor(76.3076, grad_fn=<ToCopyBackward0>)\n",
      "839: 76.30760955810547\n",
      "tensor(60.6005, grad_fn=<ToCopyBackward0>)\n",
      "840: 60.60050582885742\n",
      "tensor(175.8223, grad_fn=<ToCopyBackward0>)\n",
      "841: 175.82232666015625\n",
      "tensor(70.8934, grad_fn=<ToCopyBackward0>)\n",
      "842: 70.89339447021484\n",
      "tensor(83.4792, grad_fn=<ToCopyBackward0>)\n",
      "843: 83.47918701171875\n",
      "tensor(106.3579, grad_fn=<ToCopyBackward0>)\n",
      "844: 106.35786437988281\n",
      "tensor(106.0650, grad_fn=<ToCopyBackward0>)\n",
      "845: 106.06497192382812\n",
      "tensor(31.7218, grad_fn=<ToCopyBackward0>)\n",
      "846: 31.721824645996094\n",
      "tensor(88.6005, grad_fn=<ToCopyBackward0>)\n",
      "847: 88.60050201416016\n",
      "tensor(27.1360, grad_fn=<ToCopyBackward0>)\n",
      "848: 27.136037826538086\n",
      "tensor(110.3579, grad_fn=<ToCopyBackward0>)\n",
      "849: 110.35786437988281\n",
      "tensor(136.0650, grad_fn=<ToCopyBackward0>)\n",
      "850: 136.06497192382812\n",
      "tensor(166.4081, grad_fn=<ToCopyBackward0>)\n",
      "851: 166.40811157226562\n",
      "tensor(99.4792, grad_fn=<ToCopyBackward0>)\n",
      "852: 99.47918701171875\n",
      "tensor(90.8934, grad_fn=<ToCopyBackward0>)\n",
      "853: 90.89339447021484\n",
      "tensor(20.8431, grad_fn=<ToCopyBackward0>)\n",
      "854: 20.8431453704834\n",
      "tensor(134.9436, grad_fn=<ToCopyBackward0>)\n",
      "855: 134.9436492919922\n",
      "tensor(13.6716, grad_fn=<ToCopyBackward0>)\n",
      "856: 13.6715726852417\n",
      "tensor(60.8934, grad_fn=<ToCopyBackward0>)\n",
      "857: 60.89339828491211\n",
      "tensor(147.5294, grad_fn=<ToCopyBackward0>)\n",
      "858: 147.52943420410156\n",
      "tensor(209.2868, grad_fn=<ToCopyBackward0>)\n",
      "859: 209.2867889404297\n",
      "tensor(37.6578, grad_fn=<ToCopyBackward0>)\n",
      "860: 37.65780258178711\n",
      "tensor(87.7721, grad_fn=<ToCopyBackward0>)\n",
      "861: 87.77207946777344\n",
      "tensor(124.3579, grad_fn=<ToCopyBackward0>)\n",
      "862: 124.35786437988281\n",
      "tensor(109.4792, grad_fn=<ToCopyBackward0>)\n",
      "863: 109.47918701171875\n",
      "tensor(126.9436, grad_fn=<ToCopyBackward0>)\n",
      "864: 126.94364929199219\n",
      "tensor(68.2815, grad_fn=<ToCopyBackward0>)\n",
      "865: 68.28150939941406\n",
      "tensor(90.5014, grad_fn=<ToCopyBackward0>)\n",
      "866: 90.50142669677734\n",
      "tensor(100.0650, grad_fn=<ToCopyBackward0>)\n",
      "867: 100.06497192382812\n",
      "tensor(88.0650, grad_fn=<ToCopyBackward0>)\n",
      "868: 88.06497955322266\n",
      "tensor(87.7721, grad_fn=<ToCopyBackward0>)\n",
      "869: 87.77208709716797\n",
      "tensor(110.0650, grad_fn=<ToCopyBackward0>)\n",
      "870: 110.06497955322266\n",
      "tensor(72.8934, grad_fn=<ToCopyBackward0>)\n",
      "871: 72.89340209960938\n",
      "tensor(157.5294, grad_fn=<ToCopyBackward0>)\n",
      "872: 157.52944946289062\n",
      "tensor(103.1863, grad_fn=<ToCopyBackward0>)\n",
      "873: 103.18629455566406\n",
      "tensor(96.6508, grad_fn=<ToCopyBackward0>)\n",
      "874: 96.65076446533203\n",
      "tensor(141.5294, grad_fn=<ToCopyBackward0>)\n",
      "875: 141.52944946289062\n",
      "tensor(68.6005, grad_fn=<ToCopyBackward0>)\n",
      "876: 68.60050964355469\n",
      "tensor(42.3076, grad_fn=<ToCopyBackward0>)\n",
      "877: 42.307613372802734\n",
      "tensor(165.2365, grad_fn=<ToCopyBackward0>)\n",
      "878: 165.23654174804688\n",
      "tensor(99.1863, grad_fn=<ToCopyBackward0>)\n",
      "879: 99.18629455566406\n",
      "tensor(95.7721, grad_fn=<ToCopyBackward0>)\n",
      "880: 95.77207946777344\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "881: 112.35786437988281\n",
      "tensor(97.1863, grad_fn=<ToCopyBackward0>)\n",
      "882: 97.18629455566406\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "883: 112.35786437988281\n",
      "tensor(103.1863, grad_fn=<ToCopyBackward0>)\n",
      "884: 103.18629455566406\n",
      "tensor(91.1863, grad_fn=<ToCopyBackward0>)\n",
      "885: 91.18629455566406\n",
      "tensor(38.0147, grad_fn=<ToCopyBackward0>)\n",
      "886: 38.01472091674805\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "887: 112.35786437988281\n",
      "tensor(186.7010, grad_fn=<ToCopyBackward0>)\n",
      "888: 186.7010040283203\n",
      "tensor(67.1863, grad_fn=<ToCopyBackward0>)\n",
      "889: 67.18629455566406\n",
      "tensor(74.6005, grad_fn=<ToCopyBackward0>)\n",
      "890: 74.60050201416016\n",
      "tensor(84.6005, grad_fn=<ToCopyBackward0>)\n",
      "891: 84.60050201416016\n",
      "tensor(63.1863, grad_fn=<ToCopyBackward0>)\n",
      "892: 63.1862907409668\n",
      "tensor(116.3579, grad_fn=<ToCopyBackward0>)\n",
      "893: 116.35786437988281\n",
      "tensor(119.7721, grad_fn=<ToCopyBackward0>)\n",
      "894: 119.77207946777344\n",
      "tensor(77.7218, grad_fn=<ToCopyBackward0>)\n",
      "895: 77.7218246459961\n",
      "tensor(93.7721, grad_fn=<ToCopyBackward0>)\n",
      "896: 93.77207946777344\n",
      "tensor(16.8431, grad_fn=<ToCopyBackward0>)\n",
      "897: 16.8431453704834\n",
      "tensor(61.4289, grad_fn=<ToCopyBackward0>)\n",
      "898: 61.428932189941406\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "899: 71.18629455566406\n",
      "tensor(143.0532, grad_fn=<ToCopyBackward0>)\n",
      "900: 143.05322265625\n",
      "tensor(61.1872, grad_fn=<ToCopyBackward0>)\n",
      "901: 61.18721008300781\n",
      "tensor(153.8275, grad_fn=<ToCopyBackward0>)\n",
      "902: 153.8275146484375\n",
      "tensor(92.0723, grad_fn=<ToCopyBackward0>)\n",
      "903: 92.07229614257812\n",
      "tensor(84.9017, grad_fn=<ToCopyBackward0>)\n",
      "904: 84.90174865722656\n",
      "tensor(142.9600, grad_fn=<ToCopyBackward0>)\n",
      "905: 142.9599609375\n",
      "tensor(122.9623, grad_fn=<ToCopyBackward0>)\n",
      "906: 122.96234130859375\n",
      "tensor(128.3755, grad_fn=<ToCopyBackward0>)\n",
      "907: 128.37551879882812\n",
      "tensor(231.3133, grad_fn=<ToCopyBackward0>)\n",
      "908: 231.31326293945312\n",
      "tensor(20.5548, grad_fn=<ToCopyBackward0>)\n",
      "909: 20.554805755615234\n",
      "tensor(112.3710, grad_fn=<ToCopyBackward0>)\n",
      "910: 112.3709945678711\n",
      "tensor(48.9012, grad_fn=<ToCopyBackward0>)\n",
      "911: 48.90118408203125\n",
      "tensor(84.8994, grad_fn=<ToCopyBackward0>)\n",
      "912: 84.89938354492188\n",
      "tensor(68.6045, grad_fn=<ToCopyBackward0>)\n",
      "913: 68.60450744628906\n",
      "tensor(133.7756, grad_fn=<ToCopyBackward0>)\n",
      "914: 133.77561950683594\n",
      "tensor(27.7230, grad_fn=<ToCopyBackward0>)\n",
      "915: 27.723041534423828\n",
      "tensor(48.0154, grad_fn=<ToCopyBackward0>)\n",
      "916: 48.01540756225586\n",
      "tensor(19.1362, grad_fn=<ToCopyBackward0>)\n",
      "917: 19.136232376098633\n",
      "tensor(35.7219, grad_fn=<ToCopyBackward0>)\n",
      "918: 35.72187423706055\n",
      "tensor(47.4289, grad_fn=<ToCopyBackward0>)\n",
      "919: 47.428932189941406\n",
      "tensor(209.2870, grad_fn=<ToCopyBackward0>)\n",
      "920: 209.2869873046875\n",
      "tensor(124.9441, grad_fn=<ToCopyBackward0>)\n",
      "921: 124.94407653808594\n",
      "tensor(139.2374, grad_fn=<ToCopyBackward0>)\n",
      "922: 139.2373504638672\n",
      "tensor(64.8941, grad_fn=<ToCopyBackward0>)\n",
      "923: 64.89412689208984\n",
      "tensor(137.2380, grad_fn=<ToCopyBackward0>)\n",
      "924: 137.23800659179688\n",
      "tensor(109.7733, grad_fn=<ToCopyBackward0>)\n",
      "925: 109.77333068847656\n",
      "tensor(52.8945, grad_fn=<ToCopyBackward0>)\n",
      "926: 52.894474029541016\n",
      "tensor(79.7734, grad_fn=<ToCopyBackward0>)\n",
      "927: 79.77337646484375\n",
      "tensor(96.8944, grad_fn=<ToCopyBackward0>)\n",
      "928: 96.8943862915039\n",
      "tensor(127.2379, grad_fn=<ToCopyBackward0>)\n",
      "929: 127.23794555664062\n",
      "tensor(70.8941, grad_fn=<ToCopyBackward0>)\n",
      "930: 70.89412689208984\n",
      "tensor(165.5304, grad_fn=<ToCopyBackward0>)\n",
      "931: 165.53038024902344\n",
      "tensor(23.4292, grad_fn=<ToCopyBackward0>)\n",
      "932: 23.429187774658203\n",
      "tensor(104.0653, grad_fn=<ToCopyBackward0>)\n",
      "933: 104.06532287597656\n",
      "tensor(112.3581, grad_fn=<ToCopyBackward0>)\n",
      "934: 112.35807800292969\n",
      "tensor(0.7929, grad_fn=<ToCopyBackward0>)\n",
      "935: 0.7928909063339233\n",
      "tensor(122.9437, grad_fn=<ToCopyBackward0>)\n",
      "936: 122.94369506835938\n",
      "tensor(93.4792, grad_fn=<ToCopyBackward0>)\n",
      "937: 93.47918701171875\n",
      "tensor(55.1863, grad_fn=<ToCopyBackward0>)\n",
      "938: 55.1862907409668\n",
      "tensor(35.4289, grad_fn=<ToCopyBackward0>)\n",
      "939: 35.42893981933594\n",
      "tensor(124.3579, grad_fn=<ToCopyBackward0>)\n",
      "940: 124.35791778564453\n",
      "tensor(109.4793, grad_fn=<ToCopyBackward0>)\n",
      "941: 109.47926330566406\n",
      "tensor(99.1864, grad_fn=<ToCopyBackward0>)\n",
      "942: 99.18639373779297\n",
      "tensor(35.1361, grad_fn=<ToCopyBackward0>)\n",
      "943: 35.136104583740234\n",
      "tensor(77.7219, grad_fn=<ToCopyBackward0>)\n",
      "944: 77.721923828125\n",
      "tensor(77.4794, grad_fn=<ToCopyBackward0>)\n",
      "945: 77.47935485839844\n",
      "tensor(90.6006, grad_fn=<ToCopyBackward0>)\n",
      "946: 90.60064697265625\n",
      "tensor(132.3581, grad_fn=<ToCopyBackward0>)\n",
      "947: 132.3580780029297\n",
      "tensor(13.9645, grad_fn=<ToCopyBackward0>)\n",
      "948: 13.964502334594727\n",
      "tensor(39.1361, grad_fn=<ToCopyBackward0>)\n",
      "949: 39.1361083984375\n",
      "tensor(104.6509, grad_fn=<ToCopyBackward0>)\n",
      "950: 104.65091705322266\n",
      "tensor(108.3580, grad_fn=<ToCopyBackward0>)\n",
      "951: 108.35798645019531\n",
      "tensor(38.0148, grad_fn=<ToCopyBackward0>)\n",
      "952: 38.014774322509766\n",
      "tensor(92.0650, grad_fn=<ToCopyBackward0>)\n",
      "953: 92.06503295898438\n",
      "tensor(117.1863, grad_fn=<ToCopyBackward0>)\n",
      "954: 117.18632507324219\n",
      "tensor(213.2868, grad_fn=<ToCopyBackward0>)\n",
      "955: 213.28683471679688\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "956: 102.06497955322266\n",
      "tensor(75.7218, grad_fn=<ToCopyBackward0>)\n",
      "957: 75.7218246459961\n",
      "tensor(48.6005, grad_fn=<ToCopyBackward0>)\n",
      "958: 48.60050582885742\n",
      "tensor(69.4792, grad_fn=<ToCopyBackward0>)\n",
      "959: 69.47918701171875\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "960: 71.18629455566406\n",
      "tensor(60.8934, grad_fn=<ToCopyBackward0>)\n",
      "961: 60.89340591430664\n",
      "tensor(52.4469, grad_fn=<ToCopyBackward0>)\n",
      "962: 52.44687271118164\n",
      "tensor(109.7721, grad_fn=<ToCopyBackward0>)\n",
      "963: 109.7720947265625\n",
      "tensor(137.5295, grad_fn=<ToCopyBackward0>)\n",
      "964: 137.5294647216797\n",
      "tensor(200.7010, grad_fn=<ToCopyBackward0>)\n",
      "965: 200.70103454589844\n",
      "tensor(71.1863, grad_fn=<ToCopyBackward0>)\n",
      "966: 71.1863021850586\n",
      "tensor(8.2574, grad_fn=<ToCopyBackward0>)\n",
      "967: 8.257362365722656\n",
      "tensor(168.1152, grad_fn=<ToCopyBackward0>)\n",
      "968: 168.115234375\n",
      "tensor(29.1360, grad_fn=<ToCopyBackward0>)\n",
      "969: 29.13604164123535\n",
      "tensor(146.9436, grad_fn=<ToCopyBackward0>)\n",
      "970: 146.9436492919922\n",
      "tensor(98.3579, grad_fn=<ToCopyBackward0>)\n",
      "971: 98.35786437988281\n",
      "tensor(168.1152, grad_fn=<ToCopyBackward0>)\n",
      "972: 168.11521911621094\n",
      "tensor(37.7218, grad_fn=<ToCopyBackward0>)\n",
      "973: 37.721824645996094\n",
      "tensor(132.9436, grad_fn=<ToCopyBackward0>)\n",
      "974: 132.9436492919922\n",
      "tensor(81.4792, grad_fn=<ToCopyBackward0>)\n",
      "975: 81.47918701171875\n",
      "tensor(118.3579, grad_fn=<ToCopyBackward0>)\n",
      "976: 118.35786437988281\n",
      "tensor(156.6508, grad_fn=<ToCopyBackward0>)\n",
      "977: 156.6507568359375\n",
      "tensor(90.0650, grad_fn=<ToCopyBackward0>)\n",
      "978: 90.06497192382812\n",
      "tensor(143.2365, grad_fn=<ToCopyBackward0>)\n",
      "979: 143.23654174804688\n",
      "tensor(46.3076, grad_fn=<ToCopyBackward0>)\n",
      "980: 46.307613372802734\n",
      "tensor(31.7218, grad_fn=<ToCopyBackward0>)\n",
      "981: 31.721826553344727\n",
      "tensor(152.6508, grad_fn=<ToCopyBackward0>)\n",
      "982: 152.6507568359375\n",
      "tensor(17.1360, grad_fn=<ToCopyBackward0>)\n",
      "983: 17.13603973388672\n",
      "tensor(100.3579, grad_fn=<ToCopyBackward0>)\n",
      "984: 100.35786437988281\n",
      "tensor(97.7721, grad_fn=<ToCopyBackward0>)\n",
      "985: 97.77207946777344\n",
      "tensor(16.8431, grad_fn=<ToCopyBackward0>)\n",
      "986: 16.8431453704834\n",
      "tensor(165.2365, grad_fn=<ToCopyBackward0>)\n",
      "987: 165.23654174804688\n",
      "tensor(174.4081, grad_fn=<ToCopyBackward0>)\n",
      "988: 174.40811157226562\n",
      "tensor(88.3579, grad_fn=<ToCopyBackward0>)\n",
      "989: 88.35786437988281\n",
      "tensor(67.4792, grad_fn=<ToCopyBackward0>)\n",
      "990: 67.47918701171875\n",
      "tensor(97.4792, grad_fn=<ToCopyBackward0>)\n",
      "991: 97.47918701171875\n",
      "tensor(50.6005, grad_fn=<ToCopyBackward0>)\n",
      "992: 50.60050582885742\n",
      "tensor(225.5797, grad_fn=<ToCopyBackward0>)\n",
      "993: 225.57969665527344\n",
      "tensor(108.3579, grad_fn=<ToCopyBackward0>)\n",
      "994: 108.35786437988281\n",
      "tensor(177.8223, grad_fn=<ToCopyBackward0>)\n",
      "995: 177.82232666015625\n",
      "tensor(80.3076, grad_fn=<ToCopyBackward0>)\n",
      "996: 80.30760955810547\n",
      "tensor(46.3076, grad_fn=<ToCopyBackward0>)\n",
      "997: 46.307613372802734\n",
      "tensor(116.9436, grad_fn=<ToCopyBackward0>)\n",
      "998: 116.94364929199219\n",
      "tensor(9.9645, grad_fn=<ToCopyBackward0>)\n",
      "999: 9.964466094970703\n",
      "tensor(129.5294, grad_fn=<ToCopyBackward0>)\n",
      "1000: 129.52943420410156\n",
      "tensor(46.3076, grad_fn=<ToCopyBackward0>)\n",
      "1001: 46.307613372802734\n",
      "tensor(95.4792, grad_fn=<ToCopyBackward0>)\n",
      "1002: 95.47918701171875\n",
      "tensor(83.4792, grad_fn=<ToCopyBackward0>)\n",
      "1003: 83.47918701171875\n",
      "tensor(82.8934, grad_fn=<ToCopyBackward0>)\n",
      "1004: 82.89339447021484\n",
      "tensor(106.0650, grad_fn=<ToCopyBackward0>)\n",
      "1005: 106.06497192382812\n",
      "tensor(146.9436, grad_fn=<ToCopyBackward0>)\n",
      "1006: 146.9436492919922\n",
      "tensor(164.1152, grad_fn=<ToCopyBackward0>)\n",
      "1007: 164.11521911621094\n",
      "tensor(46.0147, grad_fn=<ToCopyBackward0>)\n",
      "1008: 46.01471710205078\n",
      "tensor(42.0147, grad_fn=<ToCopyBackward0>)\n",
      "1009: 42.01471710205078\n",
      "tensor(133.4792, grad_fn=<ToCopyBackward0>)\n",
      "1010: 133.47918701171875\n",
      "tensor(77.1863, grad_fn=<ToCopyBackward0>)\n",
      "1011: 77.18629455566406\n",
      "tensor(5.6716, grad_fn=<ToCopyBackward0>)\n",
      "1012: 5.671572685241699\n",
      "tensor(42.3076, grad_fn=<ToCopyBackward0>)\n",
      "1013: 42.307613372802734\n",
      "tensor(103.7721, grad_fn=<ToCopyBackward0>)\n",
      "1014: 103.77207946777344\n",
      "tensor(29.1360, grad_fn=<ToCopyBackward0>)\n",
      "1015: 29.13603973388672\n",
      "tensor(124.3579, grad_fn=<ToCopyBackward0>)\n",
      "1016: 124.35786437988281\n",
      "tensor(116.9436, grad_fn=<ToCopyBackward0>)\n",
      "1017: 116.94364929199219\n",
      "tensor(153.8223, grad_fn=<ToCopyBackward0>)\n",
      "1018: 153.82232666015625\n",
      "tensor(145.8223, grad_fn=<ToCopyBackward0>)\n",
      "1019: 145.82232666015625\n",
      "tensor(121.7721, grad_fn=<ToCopyBackward0>)\n",
      "1020: 121.77207946777344\n",
      "tensor(103.7721, grad_fn=<ToCopyBackward0>)\n",
      "1021: 103.77207946777344\n",
      "tensor(101.7721, grad_fn=<ToCopyBackward0>)\n",
      "1022: 101.77207946777344\n",
      "tensor(150.9436, grad_fn=<ToCopyBackward0>)\n",
      "1023: 150.9436492919922\n",
      "tensor(22.2574, grad_fn=<ToCopyBackward0>)\n",
      "1024: 22.25735855102539\n",
      "tensor(138.9436, grad_fn=<ToCopyBackward0>)\n",
      "1025: 138.9436492919922\n",
      "tensor(122.9436, grad_fn=<ToCopyBackward0>)\n",
      "1026: 122.94364929199219\n",
      "tensor(76.6005, grad_fn=<ToCopyBackward0>)\n",
      "1027: 76.60050201416016\n",
      "tensor(208.1152, grad_fn=<ToCopyBackward0>)\n",
      "1028: 208.11521911621094\n",
      "tensor(72.8934, grad_fn=<ToCopyBackward0>)\n",
      "1029: 72.89339447021484\n",
      "tensor(96.3579, grad_fn=<ToCopyBackward0>)\n",
      "1030: 96.35786437988281\n",
      "tensor(93.1863, grad_fn=<ToCopyBackward0>)\n",
      "1031: 93.18629455566406\n",
      "tensor(56.6005, grad_fn=<ToCopyBackward0>)\n",
      "1032: 56.60050582885742\n",
      "tensor(47.1360, grad_fn=<ToCopyBackward0>)\n",
      "1033: 47.13603973388672\n",
      "tensor(147.2365, grad_fn=<ToCopyBackward0>)\n",
      "1034: 147.23654174804688\n",
      "tensor(112.3579, grad_fn=<ToCopyBackward0>)\n",
      "1035: 112.35786437988281\n",
      "tensor(102.0650, grad_fn=<ToCopyBackward0>)\n",
      "1036: 102.06497192382812\n",
      "tensor(190.7010, grad_fn=<ToCopyBackward0>)\n",
      "1037: 190.7010040283203\n",
      "tensor(192.1152, grad_fn=<ToCopyBackward0>)\n",
      "1038: 192.11521911621094\n",
      "tensor(194.1152, grad_fn=<ToCopyBackward0>)\n",
      "1039: 194.11521911621094\n",
      "tensor(89.4792, grad_fn=<ToCopyBackward0>)\n",
      "1040: 89.47918701171875\n",
      "tensor(39.4289, grad_fn=<ToCopyBackward0>)\n",
      "1041: 39.428932189941406\n",
      "tensor(83.1863, grad_fn=<ToCopyBackward0>)\n",
      "1042: 83.18629455566406\n",
      "tensor(153.5294, grad_fn=<ToCopyBackward0>)\n",
      "1043: 153.52943420410156\n",
      "tensor(110.6508, grad_fn=<ToCopyBackward0>)\n",
      "1044: 110.6507568359375\n",
      "tensor(136.3579, grad_fn=<ToCopyBackward0>)\n",
      "1045: 136.3578643798828\n",
      "tensor(108.0650, grad_fn=<ToCopyBackward0>)\n",
      "1046: 108.06497192382812\n",
      "tensor(60.3076, grad_fn=<ToCopyBackward0>)\n",
      "1047: 60.307613372802734\n",
      "tensor(209.2868, grad_fn=<ToCopyBackward0>)\n",
      "1048: 209.2867889404297\n",
      "tensor(107.1863, grad_fn=<ToCopyBackward0>)\n",
      "1049: 107.18629455566406\n",
      "tensor(168.1152, grad_fn=<ToCopyBackward0>)\n",
      "1050: 168.11521911621094\n",
      "tensor(84.6005, grad_fn=<ToCopyBackward0>)\n",
      "1051: 84.60050201416016\n",
      "tensor(67.1863, grad_fn=<ToCopyBackward0>)\n",
      "1052: 67.18629455566406\n",
      "tensor(66.8934, grad_fn=<ToCopyBackward0>)\n",
      "1053: 66.89339447021484\n",
      "tensor(110.6508, grad_fn=<ToCopyBackward0>)\n",
      "1054: 110.6507568359375\n",
      "tensor(37.7218, grad_fn=<ToCopyBackward0>)\n",
      "1055: 37.721824645996094\n",
      "tensor(64.2944, grad_fn=<ToCopyBackward0>)\n",
      "1056: 64.29443359375\n",
      "tensor(170.1152, grad_fn=<ToCopyBackward0>)\n",
      "1057: 170.11521911621094\n",
      "tensor(85.4792, grad_fn=<ToCopyBackward0>)\n",
      "1058: 85.47918701171875\n",
      "tensor(82.8934, grad_fn=<ToCopyBackward0>)\n",
      "1059: 82.89339447021484\n",
      "tensor(172.4081, grad_fn=<ToCopyBackward0>)\n",
      "1060: 172.40811157226562\n",
      "tensor(98.0650, grad_fn=<ToCopyBackward0>)\n",
      "1061: 98.06497192382812\n",
      "tensor(79.4792, grad_fn=<ToCopyBackward0>)\n",
      "1062: 79.47918701171875\n",
      "tensor(132.9436, grad_fn=<ToCopyBackward0>)\n",
      "1063: 132.9436492919922\n",
      "tensor(122.6508, grad_fn=<ToCopyBackward0>)\n",
      "1064: 122.6507568359375\n",
      "tensor(128.6508, grad_fn=<ToCopyBackward0>)\n",
      "1065: 128.6507568359375\n",
      "tensor(74.6005, grad_fn=<ToCopyBackward0>)\n",
      "1066: 74.60050201416016\n",
      "tensor(188.4081, grad_fn=<ToCopyBackward0>)\n",
      "1067: 188.40811157226562\n",
      "tensor(32.9318, grad_fn=<ToCopyBackward0>)\n",
      "1068: 32.931785583496094\n",
      "tensor(102.6508, grad_fn=<ToCopyBackward0>)\n",
      "1069: 102.6507568359375\n",
      "tensor(140.9436, grad_fn=<ToCopyBackward0>)\n",
      "1070: 140.9436492919922\n",
      "tensor(152.6508, grad_fn=<ToCopyBackward0>)\n",
      "1071: 152.6507568359375\n",
      "tensor(79.1863, grad_fn=<ToCopyBackward0>)\n",
      "1072: 79.18629455566406\n",
      "tensor(58.0147, grad_fn=<ToCopyBackward0>)\n",
      "1073: 58.01471710205078\n",
      "tensor(101.4792, grad_fn=<ToCopyBackward0>)\n",
      "1074: 101.47918701171875\n",
      "tensor(110.9436, grad_fn=<ToCopyBackward0>)\n",
      "1075: 110.94364929199219\n",
      "tensor(103.1863, grad_fn=<ToCopyBackward0>)\n",
      "1076: 103.18629455566406\n",
      "tensor(149.2365, grad_fn=<ToCopyBackward0>)\n",
      "1077: 149.23654174804688\n",
      "tensor(60.8934, grad_fn=<ToCopyBackward0>)\n",
      "1078: 60.89339828491211\n",
      "tensor(97.7721, grad_fn=<ToCopyBackward0>)\n",
      "1079: 97.77207946777344\n",
      "tensor(161.8223, grad_fn=<ToCopyBackward0>)\n",
      "1080: 161.82232666015625\n",
      "tensor(119.4792, grad_fn=<ToCopyBackward0>)\n",
      "1081: 119.47918701171875\n",
      "tensor(29.4289, grad_fn=<ToCopyBackward0>)\n",
      "1082: 29.428932189941406\n",
      "tensor(57.1863, grad_fn=<ToCopyBackward0>)\n",
      "1083: 57.1862907409668\n",
      "tensor(174.4081, grad_fn=<ToCopyBackward0>)\n",
      "1084: 174.40811157226562\n",
      "tensor(181.8223, grad_fn=<ToCopyBackward0>)\n",
      "1085: 181.82232666015625\n",
      "tensor(114.5199, grad_fn=<ToCopyBackward0>)\n",
      "1086: 114.5198974609375\n",
      "tensor(100.3579, grad_fn=<ToCopyBackward0>)\n",
      "1087: 100.35786437988281\n",
      "tensor(20.8431, grad_fn=<ToCopyBackward0>)\n",
      "1088: 20.8431453704834\n",
      "tensor(148.9436, grad_fn=<ToCopyBackward0>)\n",
      "1089: 148.9436492919922\n",
      "tensor(90.3579, grad_fn=<ToCopyBackward0>)\n",
      "1090: 90.35787200927734\n",
      "tensor(100.0650, grad_fn=<ToCopyBackward0>)\n",
      "1091: 100.06497955322266\n",
      "tensor(68.3076, grad_fn=<ToCopyBackward0>)\n",
      "1092: 68.3076171875\n",
      "tensor(141.5294, grad_fn=<ToCopyBackward0>)\n",
      "1093: 141.52944946289062\n",
      "tensor(39.7218, grad_fn=<ToCopyBackward0>)\n",
      "1094: 39.72182846069336\n",
      "tensor(90.8934, grad_fn=<ToCopyBackward0>)\n",
      "1095: 90.89340209960938\n",
      "tensor(35.4289, grad_fn=<ToCopyBackward0>)\n",
      "1096: 35.42893600463867\n",
      "tensor(14.2574, grad_fn=<ToCopyBackward0>)\n",
      "1097: 14.257360458374023\n",
      "tensor(118.9437, grad_fn=<ToCopyBackward0>)\n",
      "1098: 118.94365692138672\n",
      "tensor(132.9436, grad_fn=<ToCopyBackward0>)\n",
      "1099: 132.9436492919922\n",
      "tensor(186.7010, grad_fn=<ToCopyBackward0>)\n",
      "1100: 186.70101928710938\n",
      "tensor(64.5862, grad_fn=<ToCopyBackward0>)\n",
      "1101: 64.58618927001953\n",
      "tensor(119.7721, grad_fn=<ToCopyBackward0>)\n",
      "1102: 119.77207946777344\n",
      "tensor(78.3076, grad_fn=<ToCopyBackward0>)\n",
      "1103: 78.30760955810547\n",
      "tensor(68.8934, grad_fn=<ToCopyBackward0>)\n",
      "1104: 68.89339447021484\n",
      "tensor(83.4792, grad_fn=<ToCopyBackward0>)\n",
      "1105: 83.47918701171875\n",
      "tensor(32.8431, grad_fn=<ToCopyBackward0>)\n",
      "1106: 32.84314727783203\n",
      "tensor(106.3579, grad_fn=<ToCopyBackward0>)\n",
      "1107: 106.35786437988281\n",
      "tensor(150.3579, grad_fn=<ToCopyBackward0>)\n",
      "1108: 150.3578643798828\n",
      "tensor(122.9436, grad_fn=<ToCopyBackward0>)\n",
      "1109: 122.94364929199219\n",
      "tensor(126.9436, grad_fn=<ToCopyBackward0>)\n",
      "1110: 126.94364929199219\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shaun/PeachIceTea/train.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inputs, labels):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=9'>10</a>\u001b[0m     output \u001b[39m=\u001b[39m cost(phi, theta, inputs\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat64))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=10'>11</a>\u001b[0m     loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss_fn(output, label\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat64))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[1;32m/Users/shaun/PeachIceTea/train.ipynb Cell 6'\u001b[0m in \u001b[0;36mcost\u001b[0;34m(phi, theta, data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost\u001b[39m(phi, theta, data):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean(circuit(phi, theta, data))\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py:619\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=611'>612</a>\u001b[0m using_custom_cache \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=612'>613</a>\u001b[0m     \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__getitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=613'>614</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__setitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=614'>615</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(cache, \u001b[39m\"\u001b[39m\u001b[39m__delitem__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=615'>616</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=616'>617</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape_cached \u001b[39m=\u001b[39m using_custom_cache \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mhash \u001b[39min\u001b[39;00m cache\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=618'>619</a>\u001b[0m res \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=619'>620</a>\u001b[0m     [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtape],\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=620'>621</a>\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=621'>622</a>\u001b[0m     gradient_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_fn,\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=622'>623</a>\u001b[0m     interface\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterface,\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=623'>624</a>\u001b[0m     gradient_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_kwargs,\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=624'>625</a>\u001b[0m     override_shots\u001b[39m=\u001b[39;49moverride_shots,\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=625'>626</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_kwargs,\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=626'>627</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=628'>629</a>\u001b[0m \u001b[39mif\u001b[39;00m autograd\u001b[39m.\u001b[39misinstance(res, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=629'>630</a>\u001b[0m     \u001b[39m# If a device batch transform was applied, we need to 'unpack'\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=630'>631</a>\u001b[0m     \u001b[39m# the returned tuple/list to a float.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=637'>638</a>\u001b[0m     \u001b[39m# TODO: find a more explicit way of determining that a batch transform\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=638'>639</a>\u001b[0m     \u001b[39m# was applied.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/qnode.py?line=640'>641</a>\u001b[0m     res \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py:344\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=339'>340</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_fn(res)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=341'>342</a>\u001b[0m \u001b[39mif\u001b[39;00m gradient_fn \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbackprop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m interface \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=342'>343</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_fn(\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=343'>344</a>\u001b[0m         qml\u001b[39m.\u001b[39;49minterfaces\u001b[39m.\u001b[39;49mcache_execute(\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=344'>345</a>\u001b[0m             batch_execute, cache, return_tuple\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, expand_fn\u001b[39m=\u001b[39;49mexpand_fn\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=345'>346</a>\u001b[0m         )(tapes)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=346'>347</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=348'>349</a>\u001b[0m \u001b[39m# the default execution function is batch_execute\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=349'>350</a>\u001b[0m execute_fn \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39minterfaces\u001b[39m.\u001b[39mcache_execute(batch_execute, cache, expand_fn\u001b[39m=\u001b[39mexpand_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py:172\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=167'>168</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m (res, []) \u001b[39mif\u001b[39;00m return_tuple \u001b[39melse\u001b[39;00m res\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=169'>170</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=170'>171</a>\u001b[0m     \u001b[39m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=171'>172</a>\u001b[0m     res \u001b[39m=\u001b[39m fn(execution_tapes\u001b[39m.\u001b[39;49mvalues(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=173'>174</a>\u001b[0m final_res \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=175'>176</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, tape \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tapes):\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py:97\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=94'>95</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(tapes, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=95'>96</a>\u001b[0m     tapes \u001b[39m=\u001b[39m [expand_fn(tape) \u001b[39mfor\u001b[39;00m tape \u001b[39min\u001b[39;00m tapes]\n\u001b[0;32m---> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/interfaces/execution.py?line=96'>97</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn(tapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/contextlib.py?line=75'>76</a>\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/contextlib.py?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/contextlib.py?line=77'>78</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/contextlib.py?line=78'>79</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py:355\u001b[0m, in \u001b[0;36mQubitDevice.batch_execute\u001b[0;34m(self, circuits)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=349'>350</a>\u001b[0m \u001b[39mfor\u001b[39;00m circuit \u001b[39min\u001b[39;00m circuits:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=350'>351</a>\u001b[0m     \u001b[39m# we need to reset the device here, else it will\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=351'>352</a>\u001b[0m     \u001b[39m# not start the next computation in the zero state\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=352'>353</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=354'>355</a>\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(circuit)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=355'>356</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(res)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=357'>358</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mactive:\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py:233\u001b[0m, in \u001b[0;36mDefaultQubitTorch.execute\u001b[0;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=223'>224</a>\u001b[0m         \u001b[39mif\u001b[39;00m params_cuda_device \u001b[39m!=\u001b[39m specified_device_cuda:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=225'>226</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=226'>227</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTorch device \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_torch_device\u001b[39m}\u001b[39;00m\u001b[39m specified \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=227'>228</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mupon PennyLane device creation does not match the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=228'>229</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTorch device of the gate parameters; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=229'>230</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_torch_device\u001b[39m}\u001b[39;00m\u001b[39m will be used.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=230'>231</a>\u001b[0m             )\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit_torch.py?line=232'>233</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mexecute(circuit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py:294\u001b[0m, in \u001b[0;36mQubitDevice.execute\u001b[0;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=290'>291</a>\u001b[0m         results \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mstack(results)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=292'>293</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=293'>294</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatistics(circuit\u001b[39m.\u001b[39;49mobservables)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=295'>296</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m circuit\u001b[39m.\u001b[39mis_sampled:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=297'>298</a>\u001b[0m     ret_types \u001b[39m=\u001b[39m [m\u001b[39m.\u001b[39mreturn_type \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m circuit\u001b[39m.\u001b[39mmeasurements]\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py:472\u001b[0m, in \u001b[0;36mQubitDevice.statistics\u001b[0;34m(self, observables, shot_range, bin_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=468'>469</a>\u001b[0m \u001b[39mfor\u001b[39;00m obs \u001b[39min\u001b[39;00m observables:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=469'>470</a>\u001b[0m     \u001b[39m# Pass instances directly\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=470'>471</a>\u001b[0m     \u001b[39mif\u001b[39;00m obs\u001b[39m.\u001b[39mreturn_type \u001b[39mis\u001b[39;00m Expectation:\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=471'>472</a>\u001b[0m         results\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpval(obs, shot_range\u001b[39m=\u001b[39;49mshot_range, bin_size\u001b[39m=\u001b[39;49mbin_size))\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=473'>474</a>\u001b[0m     \u001b[39melif\u001b[39;00m obs\u001b[39m.\u001b[39mreturn_type \u001b[39mis\u001b[39;00m Variance:\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=474'>475</a>\u001b[0m         results\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvar(obs, shot_range\u001b[39m=\u001b[39mshot_range, bin_size\u001b[39m=\u001b[39mbin_size))\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py:548\u001b[0m, in \u001b[0;36mDefaultQubit.expval\u001b[0;34m(self, observable, shot_range, bin_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=543'>544</a>\u001b[0m         res \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqueeze(res)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=545'>546</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreal(res)\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=547'>548</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mexpval(observable, shot_range\u001b[39m=\u001b[39;49mshot_range, bin_size\u001b[39m=\u001b[39;49mbin_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py:926\u001b[0m, in \u001b[0;36mQubitDevice.expval\u001b[0;34m(self, observable, shot_range, bin_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=922'>923</a>\u001b[0m     \u001b[39m# the probability vector must be permuted to account for the permuted wire order of the observable\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=923'>924</a>\u001b[0m     permuted_wires \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_permute_wires(observable)\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=925'>926</a>\u001b[0m     prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability(wires\u001b[39m=\u001b[39;49mpermuted_wires)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=926'>927</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dot(eigvals, prob)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=928'>929</a>\u001b[0m \u001b[39m# estimate the ev\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py:831\u001b[0m, in \u001b[0;36mQubitDevice.probability\u001b[0;34m(self, wires, shot_range, bin_size)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=815'>816</a>\u001b[0m \u001b[39m\"\"\"Return either the analytic probability or estimated probability of\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=816'>817</a>\u001b[0m \u001b[39meach computational basis state.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=817'>818</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=826'>827</a>\u001b[0m \u001b[39m    array[float]: list of the probabilities\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=827'>828</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=829'>830</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshots \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=830'>831</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49manalytic_probability(wires\u001b[39m=\u001b[39;49mwires)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=832'>833</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_probability(wires\u001b[39m=\u001b[39mwires, shot_range\u001b[39m=\u001b[39mshot_range, bin_size\u001b[39m=\u001b[39mbin_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py:781\u001b[0m, in \u001b[0;36mDefaultQubit.analytic_probability\u001b[0;34m(self, wires)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=778'>779</a>\u001b[0m real_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_real(flat_state)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=779'>780</a>\u001b[0m imag_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_imag(flat_state)\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=780'>781</a>\u001b[0m prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarginal_prob(real_state\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m \u001b[39m+\u001b[39;49m imag_state\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m, wires)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/devices/default_qubit.py?line=781'>782</a>\u001b[0m \u001b[39mreturn\u001b[39;00m prob\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py:898\u001b[0m, in \u001b[0;36mQubitDevice.marginal_prob\u001b[0;34m(self, prob, wires)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=895'>896</a>\u001b[0m num_wires \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(device_wires)\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=896'>897</a>\u001b[0m basis_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_basis_states(num_wires)\n\u001b[0;32m--> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=897'>898</a>\u001b[0m basis_states \u001b[39m=\u001b[39m basis_states[:, np\u001b[39m.\u001b[39margsort(np\u001b[39m.\u001b[39;49margsort(device_wires))]\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=899'>900</a>\u001b[0m powers_of_two \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(device_wires))[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/pennylane/_qubit_device.py?line=900'>901</a>\u001b[0m perm \u001b[39m=\u001b[39m basis_states \u001b[39m@\u001b[39m powers_of_two\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1107\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=998'>999</a>\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=999'>1000</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margsort\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=1000'>1001</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=1001'>1002</a>\u001b[0m \u001b[39m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=1002'>1003</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=1104'>1105</a>\u001b[0m \n\u001b[1;32m   <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=1105'>1106</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=1106'>1107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margsort\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py:55\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=52'>53</a>\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=54'>55</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=56'>57</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py:44\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=41'>42</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=42'>43</a>\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=43'>44</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     <a href='file:///Users/shaun/anaconda3/envs/qiskit/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cost(phi, theta, data):\n",
    "    return torch.mean(circuit(phi, theta, data))\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.tensor(0, dtype=torch.float64)\n",
    "        for input, label in zip(inputs, labels):\n",
    "            output = cost(phi, theta, inputs.to(torch.float64))\n",
    "            loss+=loss_fn(output, label.to(torch.float64))\n",
    "        loss = loss.float()\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        print('{}: {}'.format(i, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mean(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/shaun/PeachIceTea/train.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000007?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEPOCH \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch_number \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000007?line=12'>13</a>\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000007?line=14'>15</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000007?line=16'>17</a>\u001b[0m \u001b[39m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000007?line=19'>20</a>\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "\u001b[1;32m/Users/shaun/PeachIceTea/train.ipynb Cell 7'\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=17'>18</a>\u001b[0m \u001b[39m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=18'>19</a>\u001b[0m outputs \u001b[39m=\u001b[39m cost(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=20'>21</a>\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n",
      "\u001b[1;32m/Users/shaun/PeachIceTea/train.ipynb Cell 7'\u001b[0m in \u001b[0;36mcost\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost\u001b[39m(inputs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shaun/PeachIceTea/train.ipynb#ch0000006?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmean([torch\u001b[39m.\u001b[39;49mmean(\u001b[39minput\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m \u001b[39minput\u001b[39;49m \u001b[39min\u001b[39;49;00m inputs])\u001b[39m-\u001b[39mcircuit(params)\n",
      "\u001b[0;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    \n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    \n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = cost(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        #torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62b6089cd174fcd23de5a2ba1f03f98164a35935839c99f5cfe650d529eeaec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('qiskit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
