{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "import torch\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "from torch.optim import Adam, Adagrad\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QCNN_circuit_only import QCNN\n",
    "from ansatz import ConvCirc1, PoolingCirc\n",
    "\n",
    "qcnn = QCNN(n_qubits=8, conv_ansatz=ConvCirc1(), pooling_ansatz=PoolingCirc(), stride=1)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=qcnn.total_qubit)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def circuit(params, data):\n",
    "    qcnn.construct_circuit(params, data)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, concat\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        self.data = read_csv(file_name)\n",
    "\n",
    "    def filter(self, labels):\n",
    "        data_list  = []\n",
    "        for label in labels:\n",
    "            data_list.append(self.data[self.data[\"label\"]==label])\n",
    "        self.data = concat(data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.data.iloc[index]['label']\n",
    "        image = self.data.iloc[index][1:]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1000 instances\n",
      "validation set has 1000 instances\n",
      "test set has 115 instances\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: use postprocessed data instead\n",
    "# ====================================\n",
    "#transform = transforms.Compose(\n",
    "#    [transforms.ToTensor(),\n",
    "#    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & test, download if necessary\n",
    "#training_set = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "#training_set, validation_set = random_split(training_set, [50000, 10000])\n",
    "#test_set = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
    "training_set = MyDataset('dataset_pca.csv') \n",
    "training_set.filter([0, 1])#torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "training_set, validation_set = random_split(training_set, [1000, len(training_set)-1000])\n",
    "validation_set, test_set = random_split(validation_set, [1000, len(validation_set)-1000])\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for test\n",
    "training_loader = DataLoader(training_set, batch_size=25, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=25, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=25, shuffle=False)\n",
    "\n",
    "# ====================================\n",
    "\n",
    "# Report split sizes\n",
    "print('Training set has {} instances'.format(len(training_set)))\n",
    "print('validation set has {} instances'.format(len(validation_set)))\n",
    "print('test set has {} instances'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSELoss() # TODO: choose loss function\n",
    "params = torch.rand(qcnn.Calculate_Param_Num(), requires_grad=True) # TODO: choose params length\n",
    "\n",
    "optimizer = Adam([params], lr=0.01) # TODO: choose optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ccac41ac6d4d40974356a489c3caa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72efc96f2d44340a77387c96f115001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "loggername = f'{str(datetime.datetime.now())}'\n",
    "filepath = Path(f'./experiment_results/{loggername}')\n",
    "filepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def estimated_label(params, data):\n",
    "    return circuit(params, data) # TODO: customize your extimated label\n",
    "\n",
    "writer = SummaryWriter(log_dir='./runs')\n",
    "\n",
    "loss_list = []\n",
    "avg_loss_list = []\n",
    "avg_vloss_list = []\n",
    "history_of_parameters = []\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(training_loader), total=len(training_loader), desc=f\"{epoch+1}\"):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.tensor(0, dtype=torch.float64)\n",
    "        for input, label in zip(inputs, labels):\n",
    "            output = estimated_label(params, input.to(torch.float64))\n",
    "            loss+=loss_fn(output, 2*label.to(torch.float64)-1)/len(labels)\n",
    "        loss.backward()       \n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        loss_list.append(loss.item())\n",
    "        writer.add_scalar('local training loss', loss.item(), epoch*len(training_loader)+i)\n",
    "        np.save(filepath/f'epoch{epoch}_iter{epoch*len(training_loader)+i}', params.detach().numpy())\n",
    "        # logger.debug(\"epoch: {} batch_index: {} --- loss: {} \\t params: {}\".format(epoch+1, i+1, loss.item(), params.detach().numpy()))\n",
    "\n",
    "\n",
    "    avg_loss = avg_loss/len(training_loader)\n",
    "    avg_loss_list.append(avg_loss)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        avg_vloss = 0.0\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            vloss = torch.tensor(0, dtype=torch.float64)\n",
    "            vinputs, vlabels = vdata\n",
    "            for vinput, vlabel in zip(vinputs, vlabels):\n",
    "                voutput = estimated_label(params, vinput.to(torch.float64))\n",
    "                vloss += loss_fn(voutput, 2*vlabel.to(torch.float64)-1)/len(vlabels)\n",
    "            avg_vloss += vloss.item()\n",
    "        avg_vloss = avg_vloss/len(test_loader)\n",
    "        avg_vloss_list.append(avg_vloss)\n",
    "        history_of_parameters.append(params.numpy())\n",
    "        np.save(filepath/f'vaidataion_epoch{epoch}', params.numpy())\n",
    "    # print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and test\n",
    "    writer.add_scalar('validation Loss', avg_vloss, epoch)\n",
    "    writer.flush()\n",
    "        # print('{}: {}'.format(i, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    est_labels = []\n",
    "    true_labels = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_datas, labels = data\n",
    "        for label, test_data in zip(labels, test_datas):\n",
    "            est_labels.append(estimated_label(params, test_data).numpy()>0)\n",
    "            true_labels.append(label.numpy())\n",
    "\n",
    "    accuracy = sum(np.array(est_labels)==np.array(true_labels))/len(est_labels)\n",
    "    accuracy = accuracy.item()\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(filepath/'summary.json', 'w') as fp:\n",
    "    json.dump(dict(\n",
    "        accuracy=accuracy,\n",
    "        loss=loss_list,\n",
    "        avg_loss = avg_loss_list,\n",
    "        avg_vloss = avg_vloss_list,\n",
    "    ), fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62b6089cd174fcd23de5a2ba1f03f98164a35935839c99f5cfe650d529eeaec"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('qiskit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
